\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{subcaption}
\usepackage{float}
\usepackage{enumerate}

\usepackage{hyperref}
\hypersetup{colorlinks=true}

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\setlength\parindent{0pt}

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass\ (\hmwkClassInstructor\ \hmwkClassTime): \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength{\floatsep}{100pt}

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

\newenvironment{homeworkProblem}[1][]{
    \section{Problem \arabic{homeworkProblemCounter} \; \large{#1}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

\newcommand{\hmwkTitle}{Homework\ \#4}
\newcommand{\hmwkDueDate}{April 4, 2014}
\newcommand{\hmwkClass}{ComS 573}
\newcommand{\hmwkClassTime}{10am}
\newcommand{\hmwkClassInstructor}{De Brabanter}
\newcommand{\hmwkAuthorName}{Josh Davis}

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
    \vspace{0.1in}\large{\textit{Professor\ \hmwkClassInstructor\ at\ \hmwkClassTime}}
    \vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{}

\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}
\newcommand{\dx}{\mathrm{d}x}
\newcommand{\solution}{\textbf{\large Solution}}

\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\newcommand{\Std}{\mathrm{Std}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Likelihood}{\mathcal{L}}
\newcommand{\dist}[1]{\sim \mathrm{#1}}
\newcommand{\pval}{\(p\)-value}
\newcommand{\tstat}{\(t\)-statistic}

\newcommand{\X}{{\bold X}}
\newcommand{\Y}{{\bold Y}}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

\begin{document}

<<echo=FALSE, warning=FALSE, message=FALSE>>=
@

\maketitle

\pagebreak

\begin{homeworkProblem}
    From ISLR: Chapter 7, Problem 1.
    \\

    A cubic regression spline with one knot \(\xi\) can be obtained
    using a basis of the form \(1, x, x^2, x^3, (x - \xi)^3_+\) where
    \((x - \xi)^3\) if \(x > \xi\) and equals 0 otherwise. Show
    that a function of the form
    \[
        f(x)
        = \beta_0
        + \beta_1 x
        + \beta_2 x^2
        + \beta_3 x^3
        + \beta_4 (x - \xi)^3_+
    \]
    is indeed a cubic regression spline, regardless of the values of \(\beta_0,
    \beta_1, \beta_2, \beta_3, \beta_4\).
    \\

    \solution

    To solve this, we need to do four things.

    \begin{enumerate}[(a)]
        \item The first is that we need to find a cubic polynomial \(f_1(x) =
            a_1 + b_1 x + c_1 x^2 + d_1 x^3\) such that \(f(x) = f_1(x)\) for
            all \(x \leq \xi\). While expressing \(a_1, b_1, c_1, d_1\) in
            terms of \(\beta_0, \beta_1, \beta_2, \beta_3, \beta_4\).

            We also need to find a cubic polynomial \(f_2(x) = a_2 + b_2 x +
            c_2 x^2 + d_2 x^3\) such that \(f(x) = f_2(x)\) for all \(x >
            \xi\). While expressing \(a_2, b_2, c_2, d_2\) in terms of
            \(\beta_0, \beta_1, \beta_2, \beta_3, \beta_4\).

            These two functions tell us that \(f(x)\) is a piecewise polynomial.

        \item The second thing that we need to do is to show that \(f_1(\xi) =
            f_2(\xi)\).  That is, \(f(x)\) is continuous at \(\xi\).

        \item The third thing that we need to do is to show that \(f_1'(\xi) =
            f_2'(\xi)\). That is, \(f'(x)\) is continuous at \(\xi\).

        \item Lastly we need to show that \(f_1''(\xi) = f_2''(\xi)\). That is,
            \(f''(x)\) is continuous at \(\xi\).
    \end{enumerate}

    \part

    We need to find a new cubic polynomial \(f_1(x)\) such that \(f(x) =
    f_1(x)\) for all \(x \leq \xi\). Given the positive constraint on the 4th basis
    of \((x - \xi)^3_+\), we know that if \(x \geq \xi\), then the basis is equal to \(0\).
    \\

    Thus for \(f_1(x) = f(x)\), if we let \(a_1 = \beta_0, b_1 = \beta_1, c_1 = \beta_2, d_1 = \beta_3\), then it is easy to see that \(f_1(x) = f(x)\) because:
    \[
        \begin{split}
            f(x)
            &= \beta_0
            + \beta_1 x
            + \beta_2 x^2
            + \beta_3 x^3
            + \beta_4 (x - \xi)^3_+
            \\
            &= \beta_0
            + \beta_1 x
            + \beta_2 x^2
            + \beta_3 x^3
            + 0 \mbox{ because } x \leq \xi
            \\
            &= a_1
            + b_1 x
            + c_1 x^2
            + d_1 x^3
        \end{split}
    \]

    Therefore \(f_1(x) = f(x)\) for all \(x \leq \xi\).
    \\

    Now we must do the same but instead find a cubic polynomial \(f_2(x) = a_2
    + b_2 x + c_2 x^2 + d_2 x^3\) such that \(f(x) = f_2(x)\) for all \(x \leq
    \xi\).
    \\

    This gives us:
    \[
        \begin{split}
            f(x)
            &= \beta_0
            + \beta_1 x
            + \beta_2 x^2
            + \beta_3 x^3
            + \beta_4 (x - \xi)^3_+
            \\
            &= \beta_0
            + \beta_1 x
            + \beta_2 x^2
            + \beta_3 x^3
            + \beta_4 (x - \xi)^3 \quad \mbox{ because } x > \xi
            \\
            &= \beta_0
            + \beta_1 x
            + \beta_2 x^2
            + \beta_3 x^3
            + \beta_4 (x^3 - 3 x^2 \xi + 3 x \xi^2 - \xi^3)
            \\
            &= \beta_0
            + \beta_1 x
            + \beta_2 x^2
            + \beta_3 x^3
            + \beta_4 x^3 - 3 \beta_4 x^2 \xi + 3 \beta_4 x \xi^2 - \beta_4 \xi^3
            \\
            &= \beta_0 - \beta_4 \xi^3
            + \beta_1 x + 3 \beta_4 x \xi^2
            + \beta_2 x^2 - 3 \beta_4 x^2 \xi
            + \beta_3 x^3 + \beta_4 x^3
            \\
            &= (\beta_0 - \beta_4 \xi^3)
            + (\beta_1 + 3 \beta_4 \xi^2) x
            + (\beta_2 - 3 \beta_4 \xi) x^2
            + (\beta_3 + \beta_4) x^3
        \end{split}
    \]
    where \(a_2 = \beta_0 - \beta_4 \xi^3\), \(b_2 = \beta_1 + 3 \beta_4
    \xi^2\), \(c_2 = \beta_2 - 3 \beta_4 \xi\), and \(d_2 = \beta_3 +
    \beta_4\).
    \\

    Thus the first part is finished and our functions, \(f_1\) and \(f_2\) are:
    \[
        \begin{split}
            f_1(x)
            &= \beta_0
            + \beta_1 x
            + \beta_2 x^2
            + \beta_3 x^3
            \\
            f_2(x)
            &= (\beta_0 - \beta_4 \xi^3)
            + (\beta_1 + 3 \beta_4 \xi^2) x
            + (\beta_2 - 3 \beta_4 \xi) x^2
            + (\beta_3 + \beta_4) x^3
        \end{split}
    \]
    \\

    \part

    Now we need to show that our cubic functions will still be continuous at
    the knots, thus we need to show that \(f_1(\xi) = f_2(\xi)\).
    \[
        \begin{split}
            f_2(\xi)
            &= (\beta_0 - \beta_4 \xi^3)
            + (\beta_1 + 3 \beta_4 \xi^2) \xi
            + (\beta_2 - 3 \beta_4 \xi) \xi^2
            + (\beta_3 + \beta_4) \xi^3
            \\
            &= \beta_0 - \beta_4 \xi^3
            + \beta_1 \xi + 3 \beta_4 \xi^3
            + \beta_2 \xi^2 - 3 \beta_4 \xi^3
            + \beta_3 \xi^3 + \beta_4 \xi^3
            \\
            &= \beta_0
            + (\beta_4 \xi^3 - \beta_4 \xi^3)
            + \beta_1 \xi
            + (3 \beta_4 \xi^3 - 3 \beta_4 \xi^3)
            + \beta_2 \xi^2
            + \beta_3 \xi^3
            \\
            &= \beta_0
            + \beta_1 \xi
            + \beta_2 \xi^2
            + \beta_3 \xi^3
            \\
            &= f_1(\xi)
        \end{split}
    \]

    Thus the two functions are continous at the knots, \(\xi\).
    \\

    \part

    Now we need to show that the first derivatives of the cubic functions will
    still be continuous at the knots, thus we need to show that \(f_1'(\xi) =
    f_2'(\xi)\).
    \\

    The derivative of \(f_1(x)\) is:
    \[
        \begin{split}
            \deriv{f_1(x)}
            &= \beta_1 + 2 \beta_2 x + 3 \beta_3 x^2
        \end{split}
    \]

    The derivative of \(f_2(x)\) is:
    \[
        \begin{split}
            \deriv{f_2(x)}
            &= (\beta_1 + 3 \beta_4 \xi^2)
            + 2 (\beta_2 - 3 \beta_4 \xi) x
            + 3 (\beta_3 + \beta_4) x^2
        \end{split}
    \]

    This gives \(f_2'(\xi)\):
    \[
        \begin{split}
            f_2'(\xi)
            &= (\beta_1 + 3 \beta_4 \xi^2)
            + 2 (\beta_2 - 3 \beta_4 \xi) \xi
            + 3 (\beta_3 + \beta_4) \xi^2
            \\
            &= \beta_1 + 3 \beta_4 \xi^2
            + 2 \beta_2 \xi - 6 \beta_4 \xi^2
            + 3 \beta_3 \xi^2 + 3 \beta_4 \xi^2
            \\
            &= \beta_1
            + 2 \beta_2 \xi
            + (3 \beta_4 \xi^2
            + 3 \beta_4 \xi^2
            - 6 \beta_4 \xi^2)
            + 3 \beta_3 \xi^2
            \\
            &= \beta_1
            + 2 \beta_2 \xi
            + 3 \beta_3 \xi^2
            \\
            &= f_1'(\xi)
        \end{split}
    \]

    Thus first derivatives are still continous at the knots, \(\xi\).
    \\

    \part

    Now we need to show that the second derivatives of the cubic functions will
    still be continuous at the knots, thus we need to show that \(f_1''(\xi) =
    f_2''(\xi)\).
    \\

    The second derivative of \(f_1(x)\) is:
    \[
        \begin{split}
            \deriv{f_1'(x)}
            &= 2 \beta_2 + 6 \beta_3 x
        \end{split}
    \]

    The second derivative of \(f_2(x)\) is:
    \[
        \begin{split}
            \deriv{f_2'(x)}
            &= 2 (\beta_2 - 3 \beta_4 \xi)
            + 6 (\beta_3 + \beta_4) x
        \end{split}
    \]

    This gives \(f_2''(\xi)\):
    \[
        \begin{split}
            f_2''(\xi)
            &= 2 (\beta_2 - 3 \beta_4 \xi)
            + 6 (\beta_3 + \beta_4) \xi
            \\
            &= 2 \beta_2 - 6 \beta_4 \xi
            + 6 \beta_3 \xi + 6 \beta_4 \xi
            \\
            &= 2 \beta_2
            + (6 \beta_4 \xi
            - 6 \beta_4 \xi)
            + 6 \beta_3 \xi
            \\
            &= 2 \beta_2
            + 6 \beta_3 \xi
            \\
            &= f_1''(\xi)
        \end{split}
    \]

    Thus the second derivatives are still continous at the knots, \(\xi\).

    \subsubsection{Conclusion}

    Thus since we meet all the criteria from the beginning of this solution, we
    know that \(f(x)\) is indeed a cubic spline.
\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    Similar to Problem 11 from ISLR: Chapter 7.
    \\

    Do an iterative approach to GAMs by repeatedly holding all but one
    coefficient estimate fixed at its current value, and update only the
    coefficient estimate using a simple linear regression. Continue the process
    until convergence -- that is, until the coefficient estimates stop
    changing. The process flow is sketched below:

    \begin{enumerate}
        \item Download the \emph{adv.dat} data set \((n = 200)\) with response
            \(Y\) and two predictors, \(X_1, X_2\) on BlackBoard.
        \item Initialize \(\hat{\beta_1}\) (estimated coefficient of \(X_1\))
            to take on a value of your choice, say 0.
        \item Keeping \(\hat{\beta_1}\) fixed, fit the model:
            \[
                Y - \hat{\beta_1} X_1 = \beta_0 + \beta_2 X_2 + e
            \]
        \item Keeping \(\hat{\beta_2}\) fixed, fit the model:
            \[
                Y - \hat{\beta_2} X_2 = \beta_0 + \beta_1 X_1 + e
            \]
    \end{enumerate}

<<cache=TRUE>>=
# Read in the data
data <- read.csv('./adv.dat')
@

    \part

    Write a for loop to repeat (3) and (4) 1,000 times. Report the estimates of
    \(\hat{\beta_0}, \hat{\beta_1}\) and \(\hat{\beta_2}\) at each iteration of
    the for loop.  Create a plot in which each of these values is displayed
    with \(\hat{\beta_0}, \hat{\beta_1}\) and \(\hat{\beta_2}\) each shown in a
    different color.
    \\

    \solution

    First let's write some helper functions:
<<cache=TRUE>>=
estimate.beta <- function (fixed, Y, X1, X2) {
    a <- Y - fixed * X1
    fit <- lm(a ~ X2)

    # Return coefficient we want
    fit$coef[2]
}

@
    Now let's try the backfitting
<<cache=TRUE>>=
N <- 1000
R <- 1:N
beta0 <- rep(NA, N)
beta1 <- rep(NA, N)
beta2 <- rep(NA, N)

# Start off with our estimate of 0
beta1[1] <- 0

for (i in R) {
    beta2[i] <- estimate.beta(beta1[i],
                                  data$Y,
                                  data$X1,
                                  data$X2)

    # Keep the lists the same size...
    if (i != 1000) {
        beta1[i + 1] <- estimate.beta(beta2[i],
                                      data$Y,
                                      data$X2,
                                      data$X1)
    }

    # Assign beta0 manually
    fit <- lm(data$Y ~ I(beta1[i] * data$X1) + I(beta2[i] * data$X2))
    beta0[i] <- fit$coef[1]
}
@

    Plotting these values gives us the following:

<<p2a, cache=TRUE, fig.pos="h", fig.height=5, echo=FALSE, fig.cap="Comparison of estimates over 1000 iterations">>=
plotPartA <- function () {
    plot(R, beta0,
         type = "l",
         xlab = "Value of Iteration",
         ylab = "Coefficients",
         ylim = c(-1, 4),
         lwd = 3,
         col = "palegreen",
         main = "Backfitting Estimates")

    lines(R, beta1,
          lwd = 3,
          col = "indianred")

    lines(R, beta2,
          lwd = 3,
          col = "lightblue")

    # Add the legend
    legend("topright",
           c("Beta0", "Beta1", "Beta2"),
           lty = 1,
           lwd = 3,
           col = c("palegreen", "indianred", "lightblue"))
}

plotPartA()
@

\pagebreak

    \part

    Compare your answers in (a) to the results of simply performing multiple
    linear regression to predict \(Y\) using \(X_1\) and \(X_2\). Use the
    \texttt{abline()} function to overlay those multiple linear regression
    coefficient esitmates on the plot obtained in (a).
    \\

    \solution

    Plotting the previous plot with the added lines as black ones.

<<p2b, cache=TRUE, fig.pos="h", fig.height=5, fig.cap="Comparison of estimates over 1000 iterations">>=
fit <- lm(data$Y ~ data$X1 + data$X2)

plotPartA()

# Plot beta0
abline(h = fit$coef[1],
       lty = 2,
       lwd = 3,
       col = "black")

# Plot beta1
abline(h = fit$coef[2],
       lty = 2,
       lwd = 3,
       col = "black")

# Plot beta1
abline(h = fit$coef[3],
       lty = 2,
       lwd = 3,
       col = "black")
@

    \part

    On this data set, how many backfitting iterations were required in order to
    obtain a ``good'' approximation to the multiple regression coefficient
    estimates? What would be a good stopping criterion?
    \\

    \solution

    It happens quite early. Below we'll determine how many iterations it took.
    \\

    One way to create a stopping criterion is to look at the difference between
    the current and previous value. Once we reach a point where it doesn't
    change for some tolerance \(h\), then we could stop.  The tolerance could
    be changed depending on the situation but in our case, let's see what ours
    looked like (starting at 2):

<<cache=TRUE>>=
beta0.diff <- rep(NA, N)
beta1.diff <- rep(NA, N)
beta2.diff <- rep(NA, N)

# Start at 2, we can't have a difference at index 1
for (i in 2:N) {
    beta0.diff[i] <- abs(beta0[i] - beta0[i - 1])
    beta1.diff[i] <- abs(beta1[i] - beta1[i - 1])
    beta2.diff[i] <- abs(beta2[i] - beta2[i - 1])
}

<<p2c, cache=TRUE, echo=FALSE, fig.pos="h", fig.height=4, fig.cap="Differences between current and previous estimate">>=
plot(R, beta0.diff,
     type = "l",
     xlab = "Iteration",
     ylab = "Difference",
     ylim = c(0, 0.06),
     xlim = c(2, 3.5),
     lwd = 3,
     col = "palegreen")

lines(R, beta1.diff,
      lwd = 3,
      col = "indianred")

lines(R, beta2.diff,
      lwd = 3,
      col = "lightblue")

legend("topright",
       c("Beta0", "Beta1", "Beta2"),
       lty = 1,
       lwd = 3,
       col = c("palegreen", "indianred", "lightblue"))
@

    Thus we can see ours converged around just 3 iterations. And in our case,
    it they converged very quickly and thus a tolerance of 0.005 would have
    ended it properly. It might be hard to determine what will make a good
    tolerance though.
\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    Show that the Nadarya-Watson estimator is equal to \textbf{local constant}
    fitting.  \textit{Hint:} Use the local polynomial cost function to start
    and adapt where necessary.
    \\

    \solution

    Solution.
\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    Show that the kernel density estimate
    \[
        \hat{f}(x) = \frac{
            1
        }{
            nh
        }
        \sum_{i = 1}^{n} K
        \left(
            \frac{
                x - X_i
            }{
                h
            }
        \right)
    \]
    with kernel \(K\) and bandwidth \(h > 0\), is a bonafide density. Did you
    really need any condition(s) on \(K\)? If so, which one(s)?
    \\

    \solution

    According to what a bona fide kernel density estimator is, it can be
    anything as long as \(K\) satisfies these two requirements:

    \begin{enumerate}
        \item \(\int_{-\infty}^{+\infty} K(x)\dx = 1\)
        \item And \(K(-x) = K(x)\) for all values of \(x\)
    \end{enumerate}

    The explanation is that the first requirement ensures that it is a valid
    probability distribution function and the second ensures that the average
    of the distribution is equal to that of the sample that was used.
\end{homeworkProblem}

\end{document}
