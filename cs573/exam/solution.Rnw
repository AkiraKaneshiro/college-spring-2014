\documentclass{article}

\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{subcaption}
\usepackage{float}
\usepackage{fancyvrb}

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass\ (\hmwkClassInstructor\ \hmwkClassTime): \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

\newenvironment{homeworkProblem}{
    \section{Problem \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

\newcommand{\hmwkTitle}{Final Exam}
\newcommand{\hmwkDueDate}{May 5, 2014}
\newcommand{\hmwkClass}{ComS 573}
\newcommand{\hmwkClassTime}{11:45am}
\newcommand{\hmwkClassInstructor}{Professor De Brabanter}
\newcommand{\hmwkAuthorName}{Josh Davis}

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
    \vspace{0.1in}\large{\textit{\hmwkClassInstructor\ at\ \hmwkClassTime}}
    \vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{}

\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}
\newcommand{\dx}{\mathrm{d}x}
\newcommand{\solution}{\textbf{\large Solution}}

\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\newcommand{\Std}{\mathrm{Std}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Likelihood}{\mathcal{L}}
\newcommand{\dist}[1]{\sim \mathrm{#1}}
\newcommand{\pval}{\(p\)-value}
\newcommand{\tstat}{\(t\)-statistic}
\DeclareMathOperator{\Tr}{trace}

\newcommand{\I}{{\bold I}}
\newcommand{\W}{{\bold W}}
\newcommand{\X}{{\bold X}}
\newcommand{\Y}{{\bold Y}}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

\begin{document}

\maketitle

\pagebreak

\setcounter{homeworkProblemCounter}{6}
\begin{homeworkProblem}
    We want to determine the posterior distribution. We know that the
    probability \(p(\beta \mid X, Y)\) is going to be proportional to this
    posterior distribution  This gives us:
    \[
        p(\beta \mid \X, \Y)
        \propto f(\Y \mid \X, \beta) p(\beta \mid \X)
        = f(\Y \mid \X, \beta) p(\beta)
    \]

    We were given the distribution of our \(\beta_i\), which is:
    \[
        p(\beta)
        = \prod_{i = 1}^{d} p(\beta_i)
        = \prod_{i = 1}^{d}
        \frac{
            1
        }{
            \sqrt{
                2c\pi
            }
        }
        \exp \left(
            - \frac{
                \beta_i^2
                }{
                    2c
                }
        \right)
        = \left(
            \frac{
                1
            }{
                \sqrt{
                    2c\pi
                }
            }
        \right)^d
        \exp \left(
            - \frac{
                1
            }{
                2c
            }
            \sum_{i = 1}^{d} \beta_i^2
        \right)
    \]

    Using our values:
    \[
        \begin{split}
            f(\Y \mid \X, \beta)p(\beta)
            &=
            \left(
                \frac{
                    1
                }{
                    \sigma \sqrt{2\pi}
                }
            \right)^n
            \exp
            \left(
                - \frac{
                    1
                }{
                    2\sigma^2
                }
                \sum_{i = 1}^{n} (Y_i - \hat{Y_i})^2
            \right)
            \left(
                \frac{
                    1
                }{
                    \sqrt{
                        2c\pi
                    }
                }
            \right)^d
            \exp \left(
                - \frac{
                    1
                }{
                    2c
                }
                \sum_{i = 1}^{d} \beta_i^2
            \right)
            \\
            &=
            \left(
                \frac{
                    1
                }{
                    \sigma \sqrt{2\pi}
                }
            \right)^n
            \left(
                \frac{
                    1
                }{
                    \sqrt{
                        2c\pi
                    }
                }
            \right)^d
            \exp
            \left(
                - \frac{
                    1
                }{
                    2\sigma^2
                }
                \sum_{i = 1}^{n}
                \left[
                    (Y_i - \hat{Y_i})^2
                \right]
                - \frac{
                    1
                }{
                    2c
                }
                \sum_{i = 1}^{d} \beta_i^2
            \right)
        \end{split}
    \]

    Let's take the log to simplify things a bit:
    \[
        \begin{split}
            \log f(\Y \mid \X, \beta)p(\beta)
            &=
            \log
            \left[
                \left(
                    \frac{
                        1
                    }{
                        \sigma \sqrt{2\pi}
                    }
                \right)^n
                \left(
                    \frac{
                        1
                    }{
                        \sqrt{
                            2c\pi
                        }
                    }
                \right)^d
                \exp
                \left(
                    - \frac{
                        1
                    }{
                        2\sigma^2
                    }
                    \sum_{i = 1}^{n}
                    \left[
                        (Y_i - \hat{Y_i})^2
                    \right]
                    - \frac{
                        1
                    }{
                        2c
                    }
                    \sum_{i = 1}^{d} \beta_i^2
                \right)
            \right]
            \\
            &=
            n
            \left(
                \frac{
                    1
                }{
                    \sigma \sqrt{2\pi}
                }
            \right)
            +
            d \left(
                \frac{
                    1
                }{
                    \sqrt{
                        2c\pi
                    }
                }
            \right)
            -
            \left(
                \frac{
                    1
                }{
                    2\sigma^2
                }
                \sum_{i = 1}^{n}
                \left[
                    (Y_i - \hat{Y_i})^2
                \right]
                + \frac{
                    1
                }{
                    2c
                }
                \sum_{i = 1}^{d} \beta_i^2
            \right)
        \end{split}
    \]

    By maximizing that, it is obvious that we want to minimize the second part,
    thus we want to minimize:
    \[
        \frac{
            1
        }{
            2\sigma^2
        }
        \left(
            \text{RSS}
            + \frac{
                \sigma^2
            }{
                c
            }
            \sum_{i = 1}^{d} \beta_i^2
        \right)
    \]

    Well all be darned, that's just the Ridge Regression formula with \(\lambda
    = \sigma^2/c\). {\tt =]}
\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    Suppose you have the following data:

    \begin{table}[ht]
        \centering
        \begin{tabular}{c | c | c | c}
            \hline
            Observation
            & \(X_1\)
            & \(X_2\)
            & Class
            \\
            \hline
            1
            & 2
            & 2
            & +1
            \\
            2
            & 2
            & -2
            & +1
            \\
            3
            & -2
            & -2
            & +1
            \\
            4
            & -2
            & 2
            & +1
            \\
            5
            & 1
            & 1
            & -1
            \\
            6
            & 1
            & 1
            & -1
            \\
            7
            & -1
            & -1
            & -1
            \\
            8
            & -1
            & 1
            & -1
            \\
            \hline
        \end{tabular}
    \end{table}

    \part

    Show that the original data is not linearly separable by making a sketch.
    \\

    \solution

<<echo=FALSE>>=
X1 <- c(2, 2, -2, -2, 1, 1, -1, -1)
X2 <- c(2, -2, -2, 2, 1, -1, -1, 1)
color <- c(rep('red', 4), rep('green', 4))
class <- c(1, 1, 1, 1, -1, -1, -1, -1)
@

    Here is a basic graph of the data where Red represents the +1 class and
    Green represents the -1 class. As we can see, there is no way to draw a
    line separating this data.

<<p7a, echo=FALSE, fig.pos="H", fig.height=5, fig.cap="Plot of the data">>=
plot(X1, X2,
     ylim = c(-4, 4),
     xlim = c(-4, 4),
     pch = 19,
     col = color)
@

    \part

    Make a sketch of the problem when applying the feature mapping \(\varphi\)
    to the original data.
    \\

    \solution

    Applying \(\varphi\) to our data, we put our \(X_1\) and \(X_2\) through
    the equation if the condition holds. If not, we just use the original data
    points. This gives us the following data:

    \begin{table}[ht]
        \centering
        \begin{tabular}{c | c | c | c | c | c | c}
            \hline
            Observation
            & \(X_1\)
            & \(X_2\)
            & Class
            & \(\sqrt{X_1^2 + X_2^2} > 2\)
            & new \(X_1\)
            & new \(X_2\)
            \\
            \hline
            1
            & 2
            & 2
            & +1
            & \(\sqrt{8} > 2\)
            & 2
            & 2
            \\
            2
            & 2
            & -2
            & +1
            & \(\sqrt{8} > 2\)
            & 10
            & 6
            \\
            3
            & -2
            & -2
            & +1
            & \(\sqrt{8} > 2\)
            & 6
            & 6
            \\
            4
            & -2
            & 2
            & +1
            & \(\sqrt{8} > 2\)
            & 6
            & 10
            \\
            5
            & 1
            & 1
            & -1
            & \(\sqrt{2} < 2\)
            & 1
            & 1
            \\
            6
            & 1
            & -1
            & -1
            & \(\sqrt{2} < 2\)
            & 1
            & -1
            \\
            7
            & -1
            & -1
            & -1
            & \(\sqrt{2} < 2\)
            & -1
            & -1
            \\
            8
            & -1
            & 1
            & -1
            & \(\sqrt{2} < 2\)
            & -1
            & 1
            \\
            \hline
        \end{tabular}
    \end{table}

<<echo=FALSE>>=
X1.space <- c(2, 10, 6, 6, 1, 1, -1, -1)
X2.space <- c(2, 6, 6, 10, 1, -1, -1, 1)
color <- c(rep('red', 4), rep('green', 4))
class <- c(1, 1, 1, 1, -1, -1, -1, -1)
@

    Here is the data after the feature mapping:

<<p7b, echo=FALSE, fig.align="center", fig.pos="H", fig.height=5, fig.width=5, fig.cap="Plot of the data in the feature space">>=
plot(X1.space, X2.space,
     ylim = c(-2, 12),
     xlim = c(-2, 12),
     pch = 19,
     col = color)
@

    \part

    Find the equation of the separating hyperplane in this feature space and
    the corresponding Lagrange multiplies \(\alpha\) both \emph{without}
    solving a QP problem. Plot the separating hyperplane on the plot from (b)
    as well.
    \\

    \solution

    Solution.
\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}

\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    Let \(\Y = (Y_1, \ldots, Y_n)^T\) and \(\hat{\Y} = \hat{m}(x_1), \ldots,
    \hat{m}(x_n))^T\). Show for the linear smoother \(\hat{\Y} = \W\Y\) that
    \[
        \sum_{i = 1}^n \Cov(\hat{Y}_i, Y_i) = \Tr(\W) \sigma^2_e
    \]
    where \(\sigma^2_e\) denotes the error variance. Explain in words what you
    have proved in terms of effective degrees of freedom.
    \\

    \solution

    First let's simplify the left side to see how
    \[
        \begin{split}
            \sum_{i = 1}^n \Cov(\hat{Y}_i, Y_i)
            &= \Tr(\Cov(\hat{Y}, Y))
            \\
            &= \Tr(\Cov(Y, Y)\X(\X^T \X)^{-1} \X^T)
            \\
            &= \Tr(\sigma^2_e \X(\X^T \X)^{-1} \X^T)
            \\
            &= \sigma^2_e \Tr(\X(\X^T \X)^{-1} \X^T)
            \\
            &= \sigma^2_e \Tr(\W)
        \end{split}
    \]

    This shows that for the linear smoother that \(\sum_{i = 1}^n
    \Cov(\hat{Y}_i, Y_i) = \Tr(\W) \sigma^2_e\) are equal.
    \\

    To see the relationship to effective degrees of freedom, let's calculate
    what the right side equals:
    \[
        \begin{split}
            \Tr(\W) \sigma^2_e
            &= \Tr(\X(\X^T \X)^{-1} \X^T) \sigma^2_e
            \\
            &= \Tr((\X^T \X) (\X^T \X)^{-1}) \sigma^2_e \quad\quad \text{by the properties of $\Tr$}
            \\
            &= \Tr(\I) \sigma^2_e \quad\quad \text{where $\I$ is an identity matrix}
            \\
            &= \sigma^2_e \sum_{i = 1}^{d} \I_{ii}
            \\
            &= \sigma^2_e (1_1 + \cdots + 1_d)
            \\
            &= \sigma^2_e (d)
        \end{split}
    \]

    This is called effective degrees of freedom because since we are performing
    regularization on our data, \(d\) becomes the number of parameters we are
    using. This is different than the entire dimensionality of the data because
    ``effectively'' only a few parameters are having an effect.
\end{homeworkProblem}

\end{document}
