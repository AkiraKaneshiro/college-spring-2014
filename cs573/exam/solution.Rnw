\documentclass{article}

\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{subcaption}
\usepackage{float}
\usepackage{fancyvrb}

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass\ (\hmwkClassInstructor\ \hmwkClassTime): \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

\newenvironment{homeworkProblem}{
    \section{Problem \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

\newcommand{\hmwkTitle}{Final Exam}
\newcommand{\hmwkDueDate}{May 5, 2014}
\newcommand{\hmwkClass}{ComS 573}
\newcommand{\hmwkClassTime}{11:45am}
\newcommand{\hmwkClassInstructor}{Professor De Brabanter}
\newcommand{\hmwkAuthorName}{Josh Davis}

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
    \vspace{0.1in}\large{\textit{\hmwkClassInstructor\ at\ \hmwkClassTime}}
    \vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{}

\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}
\newcommand{\dx}{\mathrm{d}x}
\newcommand{\solution}{\textbf{\large Solution}}

\DeclareMathOperator{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\DeclareMathOperator{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\newcommand{\Std}{\mathrm{Std}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Likelihood}{\mathcal{L}}
\newcommand{\dist}[1]{\sim \mathrm{#1}}
\newcommand{\pval}{\(p\)-value}
\newcommand{\tstat}{\(t\)-statistic}
\DeclareMathOperator{\Tr}{trace}
\DeclareMathOperator{\Sv}{sv}

\newcommand{\A}{{\bold A}}
\newcommand{\B}{{\bold B}}
\newcommand{\I}{{\bold I}}
\newcommand{\W}{{\bold W}}
\newcommand{\X}{{\bold X}}
\newcommand{\Y}{{\bold Y}}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

\begin{document}

<<echo=FALSE, warning=FALSE, message=FALSE>>=
library('e1071')
@

\maketitle

\pagebreak

\setcounter{homeworkProblemCounter}{6}
\begin{homeworkProblem}
    We want to determine the posterior distribution. We know that the
    probability \(p(\beta \mid X, Y)\) is going to be proportional to this
    posterior distribution  This gives us:
    \[
        p(\beta \mid \X, \Y)
        \propto f(\Y \mid \X, \beta) p(\beta \mid \X)
        = f(\Y \mid \X, \beta) p(\beta)
    \]

    We were given the distribution of our \(\beta_i\), which is:
    \[
        p(\beta)
        = \prod_{i = 1}^{d} p(\beta_i)
        = \prod_{i = 1}^{d}
        \frac{
            1
        }{
            \sqrt{
                2c\pi
            }
        }
        \exp \left(
            - \frac{
                \beta_i^2
                }{
                    2c
                }
        \right)
        = \left(
            \frac{
                1
            }{
                \sqrt{
                    2c\pi
                }
            }
        \right)^d
        \exp \left(
            - \frac{
                1
            }{
                2c
            }
            \sum_{i = 1}^{d} \beta_i^2
        \right)
    \]

    Using our values:
    \[
        \begin{split}
            f(\Y \mid \X, \beta)p(\beta)
            &=
            \left(
                \frac{
                    1
                }{
                    \sigma \sqrt{2\pi}
                }
            \right)^n
            \exp
            \left(
                - \frac{
                    1
                }{
                    2\sigma^2
                }
                \sum_{i = 1}^{n} (Y_i - \hat{Y_i})^2
            \right)
            \left(
                \frac{
                    1
                }{
                    \sqrt{
                        2c\pi
                    }
                }
            \right)^d
            \exp \left(
                - \frac{
                    1
                }{
                    2c
                }
                \sum_{i = 1}^{d} \beta_i^2
            \right)
            \\
            &=
            \left(
                \frac{
                    1
                }{
                    \sigma \sqrt{2\pi}
                }
            \right)^n
            \left(
                \frac{
                    1
                }{
                    \sqrt{
                        2c\pi
                    }
                }
            \right)^d
            \exp
            \left(
                - \frac{
                    1
                }{
                    2\sigma^2
                }
                \sum_{i = 1}^{n}
                \left[
                    (Y_i - \hat{Y_i})^2
                \right]
                - \frac{
                    1
                }{
                    2c
                }
                \sum_{i = 1}^{d} \beta_i^2
            \right)
        \end{split}
    \]

    Let's take the log to simplify things a bit:
    \[
        \begin{split}
            \log f(\Y \mid \X, \beta)p(\beta)
            &=
            \log
            \left[
                \left(
                    \frac{
                        1
                    }{
                        \sigma \sqrt{2\pi}
                    }
                \right)^n
                \left(
                    \frac{
                        1
                    }{
                        \sqrt{
                            2c\pi
                        }
                    }
                \right)^d
                \exp
                \left(
                    - \frac{
                        1
                    }{
                        2\sigma^2
                    }
                    \sum_{i = 1}^{n}
                    \left[
                        (Y_i - \hat{Y_i})^2
                    \right]
                    - \frac{
                        1
                    }{
                        2c
                    }
                    \sum_{i = 1}^{d} \beta_i^2
                \right)
            \right]
            \\
            &=
            n
            \left(
                \frac{
                    1
                }{
                    \sigma \sqrt{2\pi}
                }
            \right)
            +
            d \left(
                \frac{
                    1
                }{
                    \sqrt{
                        2c\pi
                    }
                }
            \right)
            -
            \left(
                \frac{
                    1
                }{
                    2\sigma^2
                }
                \sum_{i = 1}^{n}
                \left[
                    (Y_i - \hat{Y_i})^2
                \right]
                + \frac{
                    1
                }{
                    2c
                }
                \sum_{i = 1}^{d} \beta_i^2
            \right)
        \end{split}
    \]

    By maximizing that, it is obvious that we want to minimize the second part,
    thus we want to minimize:
    \[
        \frac{
            1
        }{
            2\sigma^2
        }
        \left(
            \text{RSS}
            + \frac{
                \sigma^2
            }{
                c
            }
            \sum_{i = 1}^{d} \beta_i^2
        \right)
    \]

    Well all be darned, that's just the Ridge Regression formula with \(\lambda
    = \sigma^2/c\). {\tt =]}
\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    Suppose you have the following data:

    \begin{table}[ht]
        \centering
        \begin{tabular}{c | c | c | c}
            \hline
            Observation
            & \(X_1\)
            & \(X_2\)
            & Class
            \\
            \hline
            1
            & 2
            & 2
            & +1
            \\
            2
            & 2
            & -2
            & +1
            \\
            3
            & -2
            & -2
            & +1
            \\
            4
            & -2
            & 2
            & +1
            \\
            5
            & 1
            & 1
            & -1
            \\
            6
            & 1
            & 1
            & -1
            \\
            7
            & -1
            & -1
            & -1
            \\
            8
            & -1
            & 1
            & -1
            \\
            \hline
        \end{tabular}
    \end{table}

    \part

    Show that the original data is not linearly separable by making a sketch.
    \\

    \solution

<<echo=FALSE>>=
X1 <- c(2, 2, -2, -2, 1, 1, -1, -1)
X2 <- c(2, -2, -2, 2, 1, -1, -1, 1)
color <- c(rep('red', 4), rep('green', 4))
class <- c(1, 1, 1, 1, -1, -1, -1, -1)
@

    Here is a basic graph of the data where Red represents the +1 class and
    Green represents the -1 class. As we can see, there is no way to draw a
    line separating this data.

<<p7a, echo=FALSE, fig.pos="H", fig.height=5, fig.cap="Plot of the data">>=
plot(X1, X2,
     ylim = c(-4, 4),
     xlim = c(-4, 4),
     pch = 19,
     col = color)
@

    \part

    Make a sketch of the problem when applying the feature mapping \(\varphi\)
    to the original data.
    \\

    \solution

    Applying \(\varphi\) to our data, we put our \(X_1\) and \(X_2\) through
    the equation if the condition holds. If not, we just use the original data
    points. This gives us the following data:

    \begin{table}[ht]
        \centering
        \begin{tabular}{c | c | c | c | c | c | c}
            \hline
            Observation
            & \(X_1\)
            & \(X_2\)
            & Class
            & \(\sqrt{X_1^2 + X_2^2} > 2\)
            & new \(X_1\)
            & new \(X_2\)
            \\
            \hline
            1
            & 2
            & 2
            & +1
            & \(\sqrt{8} > 2\)
            & 2
            & 2
            \\
            2
            & 2
            & -2
            & +1
            & \(\sqrt{8} > 2\)
            & 10
            & 6
            \\
            3
            & -2
            & -2
            & +1
            & \(\sqrt{8} > 2\)
            & 6
            & 6
            \\
            4
            & -2
            & 2
            & +1
            & \(\sqrt{8} > 2\)
            & 6
            & 10
            \\
            5
            & 1
            & 1
            & -1
            & \(\sqrt{2} < 2\)
            & 1
            & 1
            \\
            6
            & 1
            & -1
            & -1
            & \(\sqrt{2} < 2\)
            & 1
            & -1
            \\
            7
            & -1
            & -1
            & -1
            & \(\sqrt{2} < 2\)
            & -1
            & -1
            \\
            8
            & -1
            & 1
            & -1
            & \(\sqrt{2} < 2\)
            & -1
            & 1
            \\
            \hline
        \end{tabular}
    \end{table}

<<echo=FALSE>>=
X1.space <- c(2, 10, 6, 6, 1, 1, -1, -1)
X2.space <- c(2, 6, 6, 10, 1, -1, -1, 1)
color <- c(rep('red', 4), rep('green', 4))
class <- c(1, 1, 1, 1, -1, -1, -1, -1)
@

    Here is the data after the feature mapping:

<<p7b, echo=FALSE, fig.align="center", fig.pos="H", fig.height=5, fig.width=5, fig.cap="Plot of the data in the feature space">>=
plot(X1.space, X2.space,
     ylim = c(-2, 12),
     xlim = c(-2, 12),
     pch = 19,
     col = color)
@

    \part

    Find the equation of the separating hyperplane in this feature space and
    the corresponding Lagrange multiplies \(\alpha\) both \emph{without}
    solving a QP problem. Plot the separating hyperplane on the plot from (b)
    as well.
    \\

    \solution

    Based on the plot, we know that we don't have to solve a QP problem because
    it is obvious which points are the SVMs. In this case, we have two:
    \[
        \Sv_1 =
        \begin{pmatrix}
            1
            \\
            1
        \end{pmatrix},
        \quad
        \Sv_2 =
        \begin{pmatrix}
            2
            \\
            2
        \end{pmatrix}
    \]

    Our hyperplane is then:
    \[
        H
        = w^T x + b = 0
        = (w_1, w_2)
        \begin{pmatrix}
            X_1
            \\
            X_2
        \end{pmatrix}
        + b = 0
    \]

    For all our \(\Sv\)s, \(Y_i[w^T X_i + b] = 1\). This gives us:
    \begin{equation}
        (w_1, w_2)
        \begin{pmatrix}
            2
            \\
            2
        \end{pmatrix}
        + b = 1
        \label{eq:1}
    \end{equation}
    \begin{equation}
        (w_1, w_2)
        \begin{pmatrix}
            1
            \\
            1
        \end{pmatrix}
        + b = -1
        \label{eq:2}
    \end{equation}

    Well shucks, we have three unknowns and only two equations. We'll need
    another one. As was mentioned in lecture, we can know that the cross
    product of the vector between our two points is going to be perpendicular
    to our final plane or also parallel to \(w\). If we go with the latter,
    we get:

    \[
        \A \times \B = \vec{0}
    \]

    Using our two points, \(\Sv_1\) and \(\Sv_2\), we can calculate the vector
    as follows
    \[
        \A
        =
        \begin{pmatrix}
            \Sv_{2x} - \Sv_{1x}
            \\
            \Sv_{2y} - \Sv_{1y}
            \\
            0
        \end{pmatrix}
        =
        \begin{pmatrix}
            2 - 1
            \\
            2 - 1
            \\
            0
        \end{pmatrix}
        =
        \begin{pmatrix}
            1
            \\
            1
            \\
            0
        \end{pmatrix}
    \]

    and then using \(w\), we get our next vector
    \[
        \B =
        \begin{pmatrix}
            w_1
            \\
            w_2
            \\
            0
        \end{pmatrix}
    \]

    The cross product is then
    \[
        \A \times \B
        =
        \begin{pmatrix}
            \A_2 \B_3 - \A_3 \B_2
            \\
            \A_3 \B_1 - \A_1 \B_3
            \\
            \A_1 \B_2 - \A_2 \B_1
        \end{pmatrix}
        =
        \begin{pmatrix}
            0
            \\
            0
            \\
            1 w_2 - 1 w_1
        \end{pmatrix}
        =
        \begin{pmatrix}
            0
            \\
            0
            \\
            0
        \end{pmatrix}
    \]

    This gives us our third equation
    \begin{equation}
        w_2 = w_1
        \label{eq:3}
    \end{equation}

    We can then subtract~\eqref{eq:2} from~\eqref{eq:1}. Which will give us:
    \[
        (2 w_1 + 2 w_2 - 1) - (w_1 + w_1 + 1)
        = w_1 + w_2 = 2
    \]

    Then using~\eqref{eq:3} we can see that \(w_1 = 1\) and \(w_2 = 1\). Then
    substituting these back into~\eqref{eq:1} gives:
    \[
        2 w_1 + 2 w_2 + b
        = 2 + 2 + b
        = 4 + b
        = 1
    \]

    Thus \(b = -3\).
    \\

    Using our Lagrangian formula where \(w = \sum_{i = 1}^n \alpha_i Y_i x_i\),
    this gives:
    \[
        \begin{pmatrix}
            w_1
            \\
            w_2
        \end{pmatrix}
        =
        \begin{pmatrix}
            1
            \\
            1
        \end{pmatrix}
        =
        \alpha_1 Y_1 x_1
        +
        \alpha_2 Y_2 x_2
        =
        -\alpha_1
        \begin{pmatrix}
            1
            \\
            1
        \end{pmatrix}
        +
        \alpha_2
        \begin{pmatrix}
            2
            \\
            2
        \end{pmatrix}
    \]

    Knowing this, we can then use our last Lagrangian where \(\alpha_1 Y_1 +
    \alpha_2 + Y_2 = 0\) and get two equations:
    \begin{equation}
        2 \alpha_2 - \alpha_1 = 1
        \label{eq:a1}
    \end{equation}
    \begin{equation}
        \alpha_2 = \alpha_1
        \label{eq:a2}
    \end{equation}

    Solving these gives us \(\alpha_1 = 1\) and \(\alpha_2 = 1\).
    \\

    Graphing our hyperplane on our previous plot we get:

<<p7c, echo=FALSE, fig.align="center", fig.pos="H", fig.height=5, fig.width=5, fig.cap="Plot of our SVM">>=
d <- data.frame(X1 = X1.space, X2 = X2.space, y = as.factor(class))
svm.fit <- svm(y ~ ., data = d, kernel = "linear", cost = 10, scale = FALSE)
plot(svm.fit, d,
     xlim = c(-1, 4),
     ylim = c(-1, 8))
@
\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    Prove or disprove that least squares support vector machines for regression
    is a linear smoother. (\emph{Hint:} Start by solving the linear system for
    \(b\) and \(\alpha\)).
    \\

    \solution

    Let's derive the LSSVM like we did in class.
    \\

    Instead of using inequality constraints, let's use equality constraints
    instead. This becomes:
    \[
        \min_{w, b}
        \quad
        \frac{
            1
        }{
            2
        }
        w^T w
        + \gamma
        \frac{1}{2}
        \sum_{i = 1}^N \xi^2_i
    \]

    such that
    \[
        Y_i (w^T \varphi(x_i) + b) = 1 - \xi_i
    \]

    Our problem is no longer QP, we lose the interpretation and now we can
    solve it with a linear system.
    \\

    Let's calculate our Lagrangians:
    \[
        \Likelihood(w, b, \xi; \alpha)
        =
        \frac{
            1
        }{
            2
        }
        w^T w
        + \gamma
        \frac{1}{2}
        \sum_{i = 1}^N \xi^2_i
        -
        \sum_{k = 1}^N
        \left[
            Y_k(w^T \varphi(x_k) + b) - 1 + \xi_k
        \right]
    \]

    This gives us the following:
    \[
        \begin{split}
            \frac{\partial \Likelihood}{\partial w}
            = 0
            & \Rightarrow
            w = \sum_{k = 1}^N \alpha_k Y_k \varphi(x_k)
            \\
            \frac{\partial \Likelihood}{\partial b}
            = 0
            & \Rightarrow
            b = \sum_{k = 1}^N \alpha_k Y_k
            \\
            \frac{\partial \Likelihood}{\partial \xi_k}
            = 0
            & \Rightarrow
            \alpha_k = \gamma \xi_k
            \\
            \frac{\partial \Likelihood}{\partial \alpha_k}
            = 0
            & \Rightarrow
            1 - \xi_k = Y_i (w^T \varphi(x_i) + b)
        \end{split}
    \]

    Then the linear system becomes:
    \[
        \begin{bmatrix}
            0 & \Y^T
            \\
            \Y & \Omega + \frac{\I}{\gamma}
        \end{bmatrix}
        \begin{bmatrix}
            b
            \\
            \alpha
        \end{bmatrix}
        =
        \begin{bmatrix}
            0
            \\
            1_n
        \end{bmatrix}
    \]

    Using Linear Algebra-fu, we get
    \[
        b = \frac{
            1^T (\Omega + \frac{1}{\gamma} \I_N) ^{-1} Y
        }{
            1^T (\Omega + \frac{1}{\gamma} \I_N) ^{-1} 1_N
        }
    \]
    \[
        a = (Y - 1_n b) (\Omega + \frac{1}{\gamma}\I_n)^{-1}
    \]

    Like we mentioned in class, our model becomes:

    \[
        f(x)
        =
        \sum_{k = 1}^N \alpha K(x, x_k) + b
    \]

    where we use a kernel \(K(x_k, x_l) = \varphi(x_k)^T \varphi(x_l)\).
    \\

    I'm in over my head because my linear algebra-fu is lacking. But if we were
    to continue, we'd see that our model is a linear combination of \(Y\). This
    is a linear smoother and we'd be done.
\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    Let \(\Y = (Y_1, \ldots, Y_n)^T\) and \(\hat{\Y} = \hat{m}(x_1), \ldots,
    \hat{m}(x_n))^T\). Show for the linear smoother \(\hat{\Y} = \W\Y\) that
    \[
        \sum_{i = 1}^n \Cov(\hat{Y}_i, Y_i) = \Tr(\W) \sigma^2_e
    \]
    where \(\sigma^2_e\) denotes the error variance. Explain in words what you
    have proved in terms of effective degrees of freedom.
    \\

    \solution

    First let's simplify the left side to see how
    \[
        \begin{split}
            \sum_{i = 1}^n \Cov(\hat{Y}_i, Y_i)
            &= \Tr(\Cov(\hat{Y}, Y))
            \\
            &= \Tr(\Cov(Y, Y)\X(\X^T \X)^{-1} \X^T)
            \\
            &= \Tr(\sigma^2_e \X(\X^T \X)^{-1} \X^T)
            \\
            &= \sigma^2_e \Tr(\X(\X^T \X)^{-1} \X^T)
            \\
            &= \sigma^2_e \Tr(\W)
        \end{split}
    \]

    This shows that for the linear smoother that \(\sum_{i = 1}^n
    \Cov(\hat{Y}_i, Y_i) = \Tr(\W) \sigma^2_e\) are equal.
    \\

    To see the relationship to effective degrees of freedom, let's calculate
    what the right side equals:
    \[
        \begin{split}
            \Tr(\W) \sigma^2_e
            &= \Tr(\X(\X^T \X)^{-1} \X^T) \sigma^2_e
            \\
            &= \Tr((\X^T \X) (\X^T \X)^{-1}) \sigma^2_e \quad\quad \text{by the properties of $\Tr$}
            \\
            &= \Tr(\I) \sigma^2_e \quad\quad \text{where $\I$ is an identity matrix}
            \\
            &= \sigma^2_e \sum_{i = 1}^{d} \I_{ii}
            \\
            &= \sigma^2_e (1_1 + \cdots + 1_d)
            \\
            &= \sigma^2_e (d)
        \end{split}
    \]

    This is called effective degrees of freedom because since we are performing
    regularization on our data, \(d\) becomes the number of parameters we are
    using. This is different than the entire dimensionality of the data because
    ``effectively'' only a few parameters are having an effect.
\end{homeworkProblem}

\end{document}
