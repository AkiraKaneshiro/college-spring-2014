\documentclass{article}

\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{extramarks}
\usepackage[usenames,dvipsnames]{color}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{changepage}
\usepackage{lineno}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{listings}
\usepackage{graphics}
\usepackage{subcaption}
\usepackage{float}
\usepackage{fancyvrb}

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass\ (\hmwkClassInstructor\ \hmwkClassTime): \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength{\floatsep}{100pt}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\algrenewcomment[1]{\hfill // #1}

\setlength\parindent{0pt}

\hypersetup{colorlinks=true}

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

\newenvironment{homeworkProblem}{
    \section{Problem \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

\newcommand{\hmwkTitle}{Homework\ \#1}
\newcommand{\hmwkDueDate}{February 7, 2014}
\newcommand{\hmwkClass}{ComS 573}
\newcommand{\hmwkClassTime}{10am}
\newcommand{\hmwkClassInstructor}{Professor De Brabanter}
\newcommand{\hmwkAuthorName}{Josh Davis}

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
    \vspace{0.1in}\large{\textit{\hmwkClassInstructor\ at\ \hmwkClassTime}}
    \vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{}

\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}
\newcommand{\dx}{\mathrm{d}x}
\newcommand{\solution}{\textbf{\large Solution}}

\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

\begin{document}

\maketitle

\pagebreak

\begin{homeworkProblem}
    Answer the following questions using the table below.

    \begin{table}[ht]
        \centering
        \begin{tabular}{c | c c c | c}
            Observation & \(X_1\) & \(X_2\) & \(X_3\) & \(Y\) \\
            \hline
            1 & 0 & 3 & 0 & Red
            \\
            2 & 2 & 0 & 0 & Red
            \\
            3 & 0 & 1 & 3 & Red
            \\
            4 & 0 & 1 & 2 & Green
            \\
            5 & -1 & 0 & 1 & Green
            \\
            6 & 1 & 1 & 1 & Red
        \end{tabular}
    \end{table}

    \part

    Compute the Euclidean distance between each observation and the test point,
    \(X_1 = X_2 = X_3 = 0\).
    \\

    \solution

    The equation for Euclidean distance is:
    \(
        \mbox{dist} = \sqrt{
            {(x_1 - 0)}^2 +
            {(x_2 - 0)}^2 +
            {(x_3 - 0)}^2
        }
    \) Thus giving us:

    \begin{table}[ht]
        \centering
        \begin{tabular}{c | c | c}
            Observation & Equation & Result \\
            1 & \(\sqrt{0^2 + 3^2 + 0^2}\) & 3
            \\
            2 & \(\sqrt{2^2 + 0^2 + 0^2}\) & 2
            \\
            3 & \(\sqrt{0^2 + 1^2 + 3^2}\) & 3.16
            \\
            4 & \(\sqrt{0^2 + 1^2 + 2^2}\) & 2.24
            \\
            5 & \(\sqrt{{-1}^2 + 0^2 + 1^2}\) & 1.41
            \\
            6 & \(\sqrt{1^2 + 1^2 + 1^2}\) & 1.73
            \\
        \end{tabular}
    \end{table}

    \part

    Prediction with \(k = 1\).
    \\

    \solution

    For \(k = 1\), the prediction for our test point includes a single
    neighbor, thus it includes Observation 5 which is Green. Since the
    probability of being Green is 1, our test point should be Green as well.
    \\

    \part

    Prediction with \(k = 3\).
    \\

    \solution

    For \(k = 3\), the prediction for our test includes three neighbors:
    Observation 5 (Green), Observation 6 (Red), and Observation 2 (Red). The
    probability for Green is then \(1/3\) and the probability of Red is
    \(2/3\). The test point should then be Red.
    \\

    \part

    If the Bayes decision boundary (gold standard) is highly nonlinear in this
    problem, then would we expect the best value for \(k\) to be large or
    small?
    \\

    \solution

    If the Bayes decision boundary is highly nonlinear, then we would expect
    the best value for \(k\) to be small. This is because the larger the value
    of \(k\), the less flexible our model becomes. The less flexible that it
    is, the more linear it gets.
\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    Suppose we would like to fit a straight line through the origin, i.e.,
    \(Y_i = \beta_1 x_i + e_i\) with \(i = 1, \ldots, n\), \(\E [e_i] = 0\),
    and \(\Var [e_i] = \sigma^2_e\) and \(\Cov[e_i, e_j] = 0, \forall i \neq
    j\).
    \\

    \part

    Find the least squares esimator for \(\hat{\beta_1}\) for the slope
    \(\beta_1\).
    \\

    \solution

    To find the least squares estimator, we should minimize our Residual Sum
    of Squares, RSS:

    \[
        \begin{split}
            RSS &= \sum_{i = 1}^{n} {(Y_i - \hat{Y_i})}^2
            \\
            &= \sum_{i = 1}^{n} {(Y_i - \hat{\beta_1} x_i)}^2
        \end{split}
    \]

    By taking the partial derivative in respect to \(\hat{\beta_1}\), we get:

    \[
        \begin{split}
            \pderiv{\hat{\beta_1}}{RSS} &= -2 \sum_{i = 1}^{n} {x_i (Y_i - \hat{\beta_1} x_i)} = 0
        \end{split}
    \]

    This gives us:

    \[
        \begin{split}
            \sum_{i = 1}^{n} {x_i (Y_i - \hat{\beta_1} x_i)}
            &= \sum_{i = 1}^{n} {x_i Y_i} - \sum_{i = 1}^{n} \hat{\beta_1} x_i^2
            \\
            &= \sum_{i = 1}^{n} {x_i Y_i} - \hat{\beta_1}\sum_{i = 1}^{n} x_i^2
        \end{split}
    \]

    Solving for \(\hat{\beta_1}\) gives the final estimator for \(\beta_1\):

    \[
        \begin{split}
            \hat{\beta_1}
            &= \frac{
                \sum {x_i Y_i}
            }{
                \sum x_i^2
            }
        \end{split}
    \]

    \pagebreak

    \part

    Calculate the bias and the variance for the estimated slope
    \(\hat{\beta_1}\).
    \\

    \solution

    For the bias, we need to calculate the expected value
    \(\E[\hat{\beta_1}]\):

    \[
        \begin{split}
            \E[\hat{\beta_1}]
            &= \E \left[ \frac{
                \sum {x_i Y_i}
            }{
                \sum x_i^2
            }\right]
            \\
            &= \frac{
                \sum {x_i \E[Y_i]}
            }{
                \sum x_i^2
            }
            \\
            &= \frac{
                \sum {x_i (\beta_1 x_i)}
            }{
                \sum x_i^2
            }
            \\
            &= \frac{
                \sum {x_i^2 \beta_1}
            }{
                \sum x_i^2
            }
            \\
            &= \beta_1 \frac{
                \sum {x_i^2 \beta_1}
            }{
                \sum x_i^2
            }
            \\
            &= \beta_1
        \end{split}
    \]

    Thus since our estimator's expected value is \(\beta_1\), we can conclude
    that the bias of our estimator is 0.
    \\

    For the variance:

    \[
        \begin{split}
            \Var[\hat{\beta_1}]
            &= \Var \left[ \frac{
                \sum {x_i Y_i}
            }{
                \sum x_i^2
            }\right]
            \\
            &=
            \frac{
                \sum {x_i^2}
            }{
                \sum x_i^2 \sum x_i^2
            } \Var[Y_i]
            \\
            &=
            \frac{
                \sum {x_i^2}
            }{
                \sum x_i^2 \sum x_i^2
            } \Var[Y_i]
            \\
            &=
            \frac{
                1
            }{
                \sum x_i^2
            } \Var[Y_i]
            \\
            &=
            \frac{
                1
            }{
                \sum x_i^2
            } \sigma^2
            \\
            &=
            \frac{
                \sigma^2
            }{
                \sum x_i^2
            }
        \end{split}
    \]

\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    \begin{figure}[H]
        \centering
        \caption{Summary from Part I}
        \VerbatimInput{partI.txt}
    \end{figure}

    \begin{figure}[H]
        \centering
        \caption{Pairs plot from Part II}
        \includegraphics[width=.88\textwidth]{partII}
    \end{figure}


    \begin{figure}[H]
        \centering
        \caption{Boxplot of Outstate vs Private in Part III}
        \includegraphics[width=.50\textwidth]{partIII}
    \end{figure}


    \begin{figure}[H]
        \centering
        \caption{Boxplot of Elite colleges vs Outstate in Part IV}
        \includegraphics[width=.50\textwidth]{partIV}
    \end{figure}

    \begin{figure}[H]
        \centering
        \caption{Various histograms from Part V}
        \includegraphics[width=.80\textwidth]{partV}
    \end{figure}

    Below is my exploration of the data. I wanted to see if there were a few
    different relationships between a few different sets of the data.
    \\

    As you can see, there seems to be a relationship between number of
    applications accepted and the percent of students coming from the top 10\%
    of high school class.
    \\

    However, there doesn't seem to be a relationship between book costs vs the
    percent of professors with a PhD.

    \begin{figure}[H]
        \centering
        \caption{Part VI Exploration}
        \includegraphics[width=.68\textwidth]{partVI}
    \end{figure}
\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    Consider the following equation of a straight line \(Y_i = \beta_0 +
    \beta_1 x_i + e_i\) with \(i = 1, \ldots, n\), \(\E[e_i] = 0\), \(\Var[e_i]
    = \sigma^2_e\), and \(\Cov[e_i, e_j] = 0, \forall i \neq j\).
    \\

    As in class, our estimator for \(\beta_1\) is:

    \[
        \hat{\beta_1}
        = \frac{
            \sum {(x_i - \bar{x})(y_i - \bar{y})}
        }{
            \sum {(x_i - \bar{x})^2}
        }
    \]

    which gives us:

    \[
        \hat{\beta_0} = \bar{y} - \hat{\beta_1}
    \]

    as the two estimators for our line as given in the book and in lecture.
    \\

    \part

    Calculate the bias for the estimator of the intercept \(\hat{\beta_0}\).
    \\

    \solution

    In class, we determined that \(\hat{\beta_1}\) is unbiased and thus
    \(\E[\hat{\beta_1}] = \beta_1\).
    \\

    Our expectation for \(\hat{\beta_0}\) is thus:

    \[
        \begin{split}
            \E[\hat{\beta_0}]
            &= \E[\bar{y} - \hat{\beta_1} \bar{x}]
            \\
            &= \E[\bar{y}] - \E[\hat{\beta_1} \bar{x}]
            \\
            &= \frac{1}{n} \sum \E[y_i] - \E[\hat{\beta_1} \bar{x}]
            \\
            &= \frac{1}{n} \sum (\beta_0 + \beta_1 x_i) - \E[\hat{\beta_1} \bar{x}]
            \\
            &= \beta_0 + \frac{1}{n} \sum (\beta_1 x_i) - \E[\hat{\beta_1} \bar{x}]
            \\
            &= \beta_0 + \beta_1 \frac{1}{n} \sum (x_i) - \E[\hat{\beta_1} \bar{x}]
            \\
            &= \beta_0 + \beta_1 \bar{x} - \E[\hat{\beta_1} \bar{x}]
            \\
            &= \beta_0 + \beta_1 \bar{x} - \beta_1 \bar{x}
            \\
            &= \beta_0
        \end{split}
    \]

    which shows that our estimator \(\hat{\beta_1}\) is unbiased.
    \\

    \pagebreak

    \part

    Calculate the variance for the estimator of the intercept \(\hat{\beta_0}\).
    \\

    \solution

    \[
        \begin{split}
            \Var[\hat{\beta_0}]
            &= \Var[\bar{y} - \hat{\beta_1} \bar{x}]
            \\
            &= \Var[\bar{y}] + \Var[- \hat{\beta_1} \bar{x}] + 2 \Cov[\bar{y}, - \hat{\beta_1} \bar{x}]
        \end{split}
    \]

    but by our assumption 3:

    \[
        \begin{split}
            \Var[\hat{\beta_0}]
            &= \Var[\bar{y}] + \Var[- \hat{\beta_1} \bar{x}] + 0
            \\
            &= \frac{1}{n^2} \sum (\Var[y_i]) + \Var[- \hat{\beta_1} \bar{x}]
            \\
            &= \frac{n \sigma^2}{n^2} + \Var[-\hat{\beta_1} \bar{x}]
            \\
            &= \frac{\sigma^2}{n} + \Var[\hat{\beta_1} \bar{x}]
            \\
            &= \frac{\sigma^2}{n} + \bar{x}^2 \Var[\hat{\beta_1}]
            \\
            &= \frac{\sigma^2}{n}
            + \bar{x}^2 \Var \left[\frac{
                \sum {(x_i - \bar{x})(y_i - \bar{y})}
            }{
                \sum {(x_i - \bar{x})^2}
            }
            \right]
            \\
            &= \frac{\sigma^2}{n}
            +
            \bar{x}^2
            \left( \frac{1}{\sum {(x_i - \bar{x})^2}} \right)^2
            \Var \left[ \sum {(x_i - \bar{x})(y_i - \bar{y})} \right]
            \\
            &= \frac{\sigma^2}{n}
            +
            \bar{x}^2
            \left( \frac{1}{\sum (x_i - \bar{x})^2} \right)^2
            \left( \sum (x_i - \bar{x})^2 \right)
            \left( \Var[y_i - \bar{y}] \right)
            \\
            &= \frac{\sigma^2}{n}
            +
            \bar{x}^2
            \left( \frac{1}{\sum (x_i - \bar{x})^2} \right)^2
            \left( \sum (x_i - \bar{x})^2 \right)
            \sigma^2
            \\
            &= \frac{\sigma^2}{n}
            +
            \bar{x}^2
            \left( \frac{1}{\sum (x_i - \bar{x})^2} \right)
            \sigma^2
            \\
            &= \frac{\sigma^2}{n}
            +
            \frac{\bar{x}^2 \sigma^2}{\sum (x_i - \bar{x})^2}
            \\
            &= \sigma^2
            \left(
                \frac{1}{n}
                +
                \frac{\bar{x}^2}{\sum (x_i - \bar{x})^2}
            \right)
        \end{split}
    \]

    which agrees with equations 3.8 on page 66 of the textbook.
\end{homeworkProblem}

\end{document}
