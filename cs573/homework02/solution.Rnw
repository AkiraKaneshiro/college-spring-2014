\documentclass{article}

\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{changepage}
\usepackage{lineno}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{listings}
\usepackage{graphics}
\usepackage{subcaption}
\usepackage{float}
\usepackage{fancyvrb}

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass\ (\hmwkClassInstructor\ \hmwkClassTime): \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength{\floatsep}{100pt}

\setlength\parindent{0pt}

\hypersetup{colorlinks=true}

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

\newenvironment{homeworkProblem}[1][]{
    \section{Problem \arabic{homeworkProblemCounter} \; \large{#1}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

\newcommand{\hmwkTitle}{Homework\ \#2}
\newcommand{\hmwkDueDate}{February 28, 2014}
\newcommand{\hmwkClass}{ComS 573}
\newcommand{\hmwkClassTime}{10am}
\newcommand{\hmwkClassInstructor}{Professor De Brabanter}
\newcommand{\hmwkAuthorName}{Josh Davis}

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
    \vspace{0.1in}\large{\textit{\hmwkClassInstructor\ at\ \hmwkClassTime}}
    \vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{}

\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}
\newcommand{\dx}{\mathrm{d}x}
\newcommand{\solution}{\textbf{\large Solution}}

\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\newcommand{\Std}{\mathrm{Std}}
\newcommand{\dist}[1]{\sim \mathrm{#1}}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

\begin{document}

<<echo=FALSE>>=
library('ISLR')
@

\maketitle

\pagebreak

\begin{homeworkProblem}
    From ISLR: Chapter 3, Problem 14.
    \\

    Using a created simulated data, answer the questions regarding simple
    linear regression.

<<>>=
# Ensure consistent values
set.seed(1)

# Create uniform distribution for first input
x1 <- runif(100)

# Normal distribution for second input
x2 <- 0.5 * x1 + rnorm(100) / 10

# Our Linear Model
y <- 2 + (2 * x1) + (.3 * x2) + rnorm(100)
@

    \part

    Write out the form of the linear model. What are the regression
    coefficients?
    \\

    \solution

    The model that we created is just \(Y = 2 + 2 X_1 + 0.3 X_2 + \epsilon\).
    Thus given we have two predictors, our model is \(Y = \beta_0 + \beta_1 X_1
    + \beta_2 X_2 + \epsilon\). Giving the values of \(\beta_0 = 2\), \(\beta_1
    = 2\), and \(\beta_2 = 0.3\).
    \\

    \part

    What is the correlation between \(X_1\) and \(X_2\)? Create a scatterplot
    displaying the relationship between the variables.
    \\

    \solution

    We can measure the correlation between the two variables by calculating the
    covariance as well as the correlation coefficient. Using our linear model, we
    can calculate the covariance as follows:

    \[
        \begin{split}
            \Cov(X_1, X_2) &= \E[X_1 X_2] - \E[X_1] \E[X_2]
        \end{split}
    \]

    This also gives us a correlation coefficient, \(\rho\) of:

    \[
        \begin{split}
            \rho = \frac{
                \Cov(X_1, X_2)
            }{
                (\Std X) (\Std Y)
            }
        \end{split}
    \]

    The scatter plot is then:

<<p1b, fig.pos="H", fig.height=4, fig.cap="Correlation of given predictors.">>=
plot(x1, x2,
     main = 'Correlation of X1 and X2',
     xlab = 'X1',
     ylab = 'X2')
@

    \part

    Using the data, fit a least squares regression to predict \(Y\) using
    \(x1\) and \(x2\). Describe the results obtained. What are
    \(\hat{\beta_0}\) and \(\hat{\beta_1}\), and \(\hat{\beta_2}\)? How do
    these relate to the true \(\beta_0\), \(\beta_1\), \(\beta_2\)? Can you
    reject the null hypothesis \(H_0 : \beta_1 = 0\)? How about \(H_0 : \beta_2
    = 0\)?
    \\

    \solution

    First let's fit a linear model to it using R's \textbf{lm()} function.

<<>>=
data <- data.frame(x1 = x1, x2 = x2, y = y)
fit <- lm(y ~ x1 + x2, data = data)
fit.coef <- coefficients(fit)
summary(fit)
@

    This gives the following estimators:

    \[
        \hat{\beta_0} = \Sexpr{fit.coef[1]},
        \quad
        \hat{\beta_1} = \Sexpr{fit.coef[2]},
        \quad
        \hat{\beta_2} = \Sexpr{fit.coef[3]}
    \]

<<p1c, fig.pos="H", fig.height=4, fig.cap="Correlation of given predictors.">>=
plot(data)
@

    \part

    Now fit a least squares regression to predict \(Y\) using only \(x1\).
    Comment on your results. Can you reject the null hypothesis \(H_0 : \beta_1
    = 0\)?
    \\

    \solution

    Fitting only \(x1\) gives us:

<<>>=
fit.x1 <- lm(y ~ x1, data = data)
fit.x1.coef <- coefficients(fit.x1)
summary(fit.x1)
@

    \part

    Now fit a least squares regression to predict \(Y\) using only \(x2\).
    Comment on your results. Can you reject the null hypothesis \(H_0 : \beta_1
    = 0\)?
    \\

    \solution

    Fitting only \(x2\) gives us:

<<>>=
fit.x2 <- lm(y ~ x2, data = data)
fit.x2.coef <- coefficients(fit.x2)
summary(fit.x2)
@

    \part

    Do the results obtained in (c-e) contradict each other? Explain your
    answer.
    \\

    \solution

    Solution.
    \\

    \part

    Now suppose we obtain one additional observation, which was unfortunately
    mismeasured.

<<>>=
x1 <- c(x1, 0.1)
x2 <- c(x2, 0.8)
y <- c(y, 6)
@

    Re-fit the lienar models from (c-e) using this new data. What effects does
    this new observation have on each of the models? In each model, is this
    observation an outlier? A high leverage point?  Both? Explain your answers
    and make suitable plots.
    \\

    \solution

    Solution.
\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    From ISLR: Chapter 4, Problem 3 (Conceptual).
    \\

    This problem relates to the QDA model, in which the obesrvations within
    each class are drawn from a normal distribution with a class specific mean
    vector and a class specific covariance matrix. We consider the simple case
    where \(p = 1\), there is only one feature. Suppose that we have \(K\)
    classes, and if an observation belongs to the \(kth\) class then \(X\)
    comes from a one-dimensional normal distribution, \(X \dist{N}(\mu_k,
    \sigma_k^2)\).  Recall that the density function for the one-dimensional
    normal distribution is given in Eq. 4.11 in the text.  Prove that in this
    case, the Bayes classifier is not linear.  Argue that it is in fact
    quadratic.
    \\

    \textit{Hint: For this problem, you should follow the arguments laid out in
    Section 4.4.2, but without making the assumption that \(\sigma_1^2 =
    \ldots = \sigma_K^2\).}
    \\

    \solution

    Solution.
\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    From ISLR: Chapter 4, Problem 7 (Conceptual).
    \\

    Suppose that you wish to predict whether a given stock will issue a
    dividend this year based on \(X\), last year's percent profit. We examine a
    large number of companies and discover that the mean value of \(X\) for
    companies is issued a dividend was \(\overline{X} = 10\), while the mean
    for those that didn't was \(\overline{X} = 0\). In addition, the variance
    of \(X\) for these two sets of companies was \(\sigma^2 = 36\). Finally,
    80\% of companies issued dividends.  Assuming that \(X\) follows a normal
    distribution, predict the probability that a company will issue a dividend
    this year given that its percentage profit was \(X = 4\) last year.
    \\

    \textit{Hint: Recall that the density function of a normal random variable
    is below. You will need to use Bayes' Theorem.}

    \[
        f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-(x - \mu)^2 / 2\sigma^2}
    \]

    \solution

    We want to predict whether or not that a company will issue a dividend this year
    given that its percentage profit was 4.
    \\

    This gives us two classes, or \(K = 2\). Where the classes are whether a
    company issued a dividend or not. We can assign values to these such that
    \(k_1 =\) issued a dividend, and \(k_2 =\) did not issue a dividend.
    \\

    By taking the values that we get out of the problem statement, we know we
    are given \(\sigma = 6\), \(\mu_1 = 10\) when issued a dividend, and
    \(\mu_2 = 0\) when a dividend isn't given. We also know that the total
    probability of companies that issued dividends, or \(\pi_1 = 0.80\) and thus
    \(\pi_2 = 0.20\).
    \\

    By using Bayes' Theorem, this gives us:

    \[
        \begin{split}
            \Pr(Y = 1 \mid X = 4)
            &= \frac{
                \pi_1 f_k(x)
            }{
                \sum_{l = 1}^K \pi_l f_l(x)
            }
            \\
            &= \frac{
                \pi_1 f_1(4)
            }{
                \sum_{l = 1}^2 \pi_l f_l(4)
            }
            \\
            &= \frac{
                \pi_1 f_1(4)
            }{
                \pi_1 f_1(4) + \pi_2 f_2(4)
            }
        \end{split}
    \]

    by substituting in our values and formulas, we get:

    \[
        \begin{split}
            \Pr(Y = 1 \mid X = 4)
            &= \frac{
                \pi_1 f_1(4)
            }{
                \pi_1 f_1(4) + \pi_2 f_2(4)
            }
            \\
            &= \frac{
                (0.80) f_1(4)
            }{
                (0.80) f_1(4) + (.20) f_2(4)
            }
            \\
            &= \frac{
                \frac{
                    0.80
                }{
                    \sqrt{2\pi (36)}
                } e^{-(4 - 10)^2/2(36)}
            }{
                \frac{
                    0.80
                }{
                    \sqrt{2\pi (36)}
                } e^{-(4 - 10)^2/2(36)}
                +
                \frac{
                    0.20
                }{
                    \sqrt{2\pi (36)}
                } e^{-(4 - 0)^2/2(36)}
            }
            \\
            &= \frac{
                (0.80) (0.0403)
            }{
                (0.80) (0.0403) + (0.20) (0.0532)
            }
            \\
            &= .7519
        \end{split}
    \]

    Thus there is a 75.2\% chance that the company will issue a dividend with a
    percentage profit of 4.
\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    From ISLR: Chapter 4, Problem 10 (Applied).
    \\

    This question should be answered using the Weekly data set, which is part
    of the ISLR package. This data is similar in nature to the \textit{Smarket}
    data from this chapter's lab, except that it contains 1,089 weekly returns
    for 21 years, from the beginning of 1990 to the end of 2010.
    \\

    \part

    Produce some numerical and graphical summaries of the Weekly data. Do there
    appear to be any patterns?
    \\

    \solution

<<>>=
summary(Weekly)
@

<<p4a, fig.pos="H", fig.height=5, fig.cap="Pairs of Weekly data.", dev="png">>=
pairs(Weekly)
@

    \part

    Use the full data set to perform a logistic regression with
    \textit{Direction} as the response and the five lag variables plus
    \textit{Volume} as predictors. Use the summary function to print the
    results. Do any of the predictors appear to be statistically significant?
    If so, which ones?
    \\

    \solution

<<>>=
fit.glm <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
              data = Weekly,
              family = binomial())
summary(fit.glm)
@

    \part

    Compute the confusion matrix and overall fraction of correct predictions.
    Explain what the confusion matrix is telling you about the types of
    mistakes made by logistic regression.
    \\

    \solution
<<>>=
prediction <- predict.glm(fit.glm,
                          Weekly,
                          type = c('response'))

classes <- rep(0, dim(Weekly)[1])
classes[prediction >= 0.5] <- 1

# Make matrix
table(Weekly$Direction, classes)
@
    Solution.
    \\

    \part

    Now fit the logistic regression model using a training data period from
    1990 to 2008, with \textit{Lag2} as the only predictor. Compute the
    confusion matrix and the overall fraction of correct predictions for the
    held out data (that is, the data from 2009 and 2010).
    \\

    \solution

    Solution.
    \\

    \part

    Repeat (d) using LDA.
    \\

    \solution

    Solution.
    \\

    \part

    Repeat (d) using QDA.
    \\

    \solution

    Solution.
    \\

    \part

    Is it justified to use QDA? Use appropriate hypothesis test(s) we've seen
    in class.
    \\

    \solution

    Solution.
    \\

    \part

    Repeat (d) using KNN with \(K = 1\).
    \\

    \solution

    Solution.
    \\

    \part

    Which of these methods appears to provide the best results on this data?
    \\

    \solution

    Solution.
    \\

    \part

    Could you create a better classifier? How would you do this?
    \\

    \solution

    Solution.
\end{homeworkProblem}

\end{document}
