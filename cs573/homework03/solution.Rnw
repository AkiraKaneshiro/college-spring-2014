\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{subcaption}
\usepackage{float}

\usepackage{hyperref}
\hypersetup{colorlinks=true}

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\setlength\parindent{0pt}

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass\ (\hmwkClassInstructor\ \hmwkClassTime): \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength{\floatsep}{100pt}

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

\newenvironment{homeworkProblem}[1][]{
    \section{Problem \arabic{homeworkProblemCounter} \; \large{#1}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

\newcommand{\hmwkTitle}{Homework\ \#3}
\newcommand{\hmwkDueDate}{March 28, 2014}
\newcommand{\hmwkClass}{ComS 573}
\newcommand{\hmwkClassTime}{10am}
\newcommand{\hmwkClassInstructor}{De Brabanter}
\newcommand{\hmwkAuthorName}{Josh Davis}

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
    \vspace{0.1in}\large{\textit{Professor\ \hmwkClassInstructor\ at\ \hmwkClassTime}}
    \vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{}

\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}
\newcommand{\dx}{\mathrm{d}x}
\newcommand{\solution}{\textbf{\large Solution}}

\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\newcommand{\Std}{\mathrm{Std}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Likelihood}{\mathcal{L}}
\newcommand{\dist}[1]{\sim \mathrm{#1}}
\newcommand{\pval}{\(p\)-value}
\newcommand{\tstat}{\(t\)-statistic}

\newcommand{\X}{{\bold X}}
\newcommand{\Y}{{\bold Y}}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

\begin{document}

<<echo=FALSE, warning=FALSE, message=FALSE>>=
library('MASS')

# For Best Subset Selection
library('leaps')

# For Ridge Regression
library('glmnet')
library('genridge')

# For Bootstrapping
library('boot')

# For PCR
library('pls')
@

\maketitle

\pagebreak

\begin{homeworkProblem}
    From ISLR: Chapter 6, Problem 7.
    \\

    Suppose that \(Y_i = \beta_0 + \sum_{j = 1}^p x_{ij} \beta_j + e_i\) where
    \(e_1, \ldots, e_n\) are i.i.d. distributed from a \(\N(0, \sigma^2_e)\).
    \\

    \part

    Write out the likelihood for the data and show that it is equivalent to
    using ordinary least squares.
    \\

    \solution

    First let's remind ourselves what the ordinary least squares solution is. It
    is the set of coefficients that minimize the residual sum of squares, or:
    \[
        \text{RSS} = \sum_{i = 1}^{n} (Y_i - \hat{Y_i})^2
    \]

    where \(\hat{Y_i} = \hat{\beta_0} + \hat{\beta_1} x_{i1} + \cdots +
    \hat{\beta_p} x_{ip}\), our model.
    \\

    To show that the maximum likelihood is equal to the least squares solution
    when the error is distributed from a normal, let's look at what maximum
    likelihood means.
    \\

    Often it is useful to calculate the statistical parameters, \(\theta\), of
    a model. However this can be hard to do given data from the model, \(X\).
    Written out, the probability of getting the data we got using the
    parameters given the data or \(\Pr[\beta \mid \theta]\).
    \\

    The idea behind the likelihood is that we want to determine the probability
    in the reverse and think about given the data that we got, what is most
    likely the values of the parameters, or \(\Likelihood(\theta \mid X)\).
    Putting this together we get:
    \[
        \Likelihood(\theta \mid \beta_1, \ldots, \beta_n)
        = P(\beta_1, \ldots, \beta_n \mid \theta)
    \]

    Given that we know these i.i.d. which means we can that our joint
    probability is just equal to product of the probabilities, thus we get:
    \\
    \[
        \begin{split}
            \Likelihood(\theta \mid \beta_1, \ldots, \beta_n)
            &= P(\beta_1 \mid \theta)
            \times \cdots
            \times P(\beta_n \mid \theta)
            \\
            &= \prod_{i = 1}^{n}
            P(\beta_i \mid \theta)
        \end{split}
    \]

    Since we know that \(e_i \dist{\N}(0, \sigma^2_e)\) and that \(e_i = Y_i -
    \hat{Y_i}\), we can use the Normal distribution probability density
    function which gives us:
    \[
        \begin{split}
            \Likelihood(\theta \mid \beta_1, \ldots, \beta_n )
            &= \prod_{i = 1}^{n}
            \frac{
                1
            }{
                \sigma \sqrt{2\pi}
            }
            \exp
            \left(-
                \frac{
                    (Y_i - \hat{Y_i})^2
                }{
                    2\sigma^2
                }
            \right)
            \\
            &= \left(
                    \frac{
                        1
                    }{
                        \sigma \sqrt{2\pi}
                    }
                \right)^n
                \prod_{i = 1}^{n}
                \exp
                \left(
                    - \frac{
                        (Y_i - \hat{Y_i})^2
                    }{
                        2\sigma^2
                    }
                \right)
        \end{split}
    \]

    To simplify the expression, we can take the log of each side. This gives us
    an expression that is easier to work with and we get:
    \[
        \begin{split}
            \log \Likelihood(\theta \mid \beta_1, \ldots, \beta_n)
            &= \log
            \left[
                \left(
                    \frac{
                        1
                    }{
                        \sigma \sqrt{2\pi}
                    }
                \right)^n
                \prod_{i = 1}^{n}
                \exp
                \left(-
                    \frac{
                        (Y_i - \hat{Y_i})^2
                    }{
                        2\sigma^2
                    }
                \right)
            \right]
            \\
            &= n \log
            \left(
                \frac{
                    1
                }{
                    \sigma \sqrt{2\pi}
                }
            \right)
            + \sum_{i = 1}^{n}
            \log
            \left(
                \exp
                \left(
                    - \frac{
                        (Y_i - \hat{Y_i})^2
                    }{
                        2\sigma^2
                    }
                \right)
            \right)
            \\
            &= n \log
            \left(
                \frac{
                    1
                }{
                    \sigma \sqrt{2\pi}
                }
            \right)
            + \sum_{i = 1}^{n}
            \left(
                - \frac{
                     (Y_i - \hat{Y_i})^2
                }{
                    2\sigma^2
                }
            \right)
            \\
            &= n \log
                \left(
                    \frac{
                        1
                    }{
                        \sigma \sqrt{2\pi}
                    }
                \right)
            - \frac{1}{2\sigma^2}
            \sum_{i = 1}^{n} (Y_i - \hat{Y_i})^2
        \end{split}
    \]

    Now since we are looking to maximize our Likelihood, or \(\Likelihood\), we
    get the following:
    \[
        \begin{split}
            \log
            \Likelihood(\theta \mid \beta_1, \ldots, \beta_n)
            &= n \log
            \left(
                \frac{
                    1
                }{
                    \sigma \sqrt{2\pi}
                }
            \right)
            - \frac{
                1
            }{
                2\sigma^2
            }
            \sum_{i = 1}^{n} (Y_i - \hat{Y_i})^2
        \end{split}
    \]

    We can see that the second term is negative, thus in order to maximize \(\Likelihood\),
    we want to minimize the second term. If we remove the constant, we then want to minimize:
    \[
        \sum_{i = 1}^n (Y_i - \hat{Y_i})^2
    \]

    Well, this is nice because this is our old friend, RSS, from the beginning
    of this problem. The RSS is what the ordinary least squares aims to
    minimize. Thus we can see that when our errors, \(e_i\), are from a Normal
    distribution, the maximum likelihood is the same as ordinary least squares.
    \\

    \part

    Assume the following prior for \(\beta\): \(\beta_1, \ldots, \beta_p\), are
    i.i.d. according to a Laplace distribution with mean zero and common scale
    parameter \(c\), i.e., \(h(\beta) = \frac{1}{2c} \exp \left(- \left\| \beta
    \right\| / c \right)\). You can assume that \(Y_i = \beta_0 + \sum_{j =
    1}^p x_{ij}\beta_j + e_i\).
    \\

    Write out the posterior for \(\beta\) in this setting. Argue that the LASSO
    estimate is the mode for \(\beta\) i.e., the most likely value for
    \(\beta\), under this posterior distribution. Determine the value for the
    parameter \(\lambda\) in the LASSO cost function.
    \\

    \solution

    We want to determine the posterior distribution. We know that the
    probability \(p(\beta \mid X, Y)\) is going to be proportaional to this
    posterior distirbution.  This gives us:
    \[
        p(\beta \mid X, Y)
        \propto f(Y \mid X, \beta) p(\beta \mid X)
        = f(Y \mid X, \beta) p(\beta)
    \]

    where \(f\) is the likelihood from part (a), and \(p(\beta)\) is our
    density. Substituting these values gives us the following:
    \[
        \begin{split}
            f(Y \mid X, \beta)p(\beta)
            &= f(Y \mid X, \beta)
            \prod_{i = 1}^{p}
            \frac{1}{2c} \exp \left(- \left\| \beta_i \right\| / c \right)
            \\
            &=
            \left(
                \frac{
                    1
                }{
                    \sigma \sqrt{2\pi}
                }
            \right)^n
            \prod_{i = 1}^{n}
            \exp
            \left(
                - \frac{
                    (Y_i - \hat{Y_i})^2
                }{
                    2\sigma^2
                }
            \right)
            \prod_{i = 1}^{p}
            \frac{1}{2c} \exp \left(- \left\| \beta_i \right\| / c \right)
        \end{split}
    \]

    \(\lambda = 2\sigma^2 / c\)

\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    Suppose we estimate some statistics (e.g. median) based on a sample \(X\).
    \\

    \part

    Carefully describe how you might estimate the standard deviation of the
    median of the statistic.
    \\

    \solution

    We want to determine the standard deviation of the median. We know that this
    median is of the population yet we only have a sample, \(X\). This looks
    like a perfect situation where we can use the Bootstrap.
    \\

    The Bootstrap works by taking some number of elements from our sample
    \textit{with replacement}. The replacement part is important because if it
    was not with replacement, we'd be introducing more variance. We do this
    re-sampling a large number of times and let this number equal \(B\). This value
    of \(B\) works best when it is in the thousands.
    \\

    Each time we pull out a new re-sampling of our original sample, \(X\), we calculate
    the median of this sample. We store this and then continue on until we are done
    with all \(B\) re-sampling. At this point, we can then calculate our standard
    deviation. And we hope that it will be close to the true value.
    \\

    A figure of this process is below:

    \begin{figure}[H]
        \includegraphics[width=\maxwidth]{bootstrap.pdf}
        \caption[Illustration of the Bootstrap]{Illustration of the Bootstrap.\label{fig:p2a}}
    \end{figure}

    \part

    Write R code that calculates the standard deviation of the median given a
    sample \(X\).
    \\

    \solution

<<>>=
# Load the data
lawstat <- read.table("lawstat.dat")

# Consistency...
set.seed(1)

# Sample obvseration indices
s <- sample(1:nrow(lawstat),
            15,
            replace = TRUE)
s

# Given data of 15 observations
X <- lawstat[s,]
L <- length(X$GPA)

#
# Manual Bootstrap
#

# Bootstrap 4k times
B <- 4000

results <- rep(0, B)

# I choose you, Bootstrap!
for (i in 1:B) {
    obs <- round(L * runif(L, 0, 1))
    sample <- X[obs,]
    results[i] <- median(sample$GPA)
}

sd(results)

#
# Using Boot Library
#

boot.fn <- function (data, indices) {
    sample <- X[indices,]
    median(sample$GPA)
}

results <- boot(data = X, statistic = boot.fn, R = B)
results

<<p2a, fig.pos="H", fig.height=6, fig.cap="Confidence interval plots">>=
plot(results)

<<>>=
# Calculate the median from the results
sd(results$t[,1])

@

    \part

    Suppose you were interested in a \(100(1 - \alpha)\) (pointwise) confidence
    interval for the correlation coefficient of a sample of \(X\) and \(Y\)
    (the joint distribution of \(X\) and \(Y\) is not bivariate normal).
    Clearly explain and derive how you would do this? Write R code that
    calculates the 95\% confidence interval for the correlation coefficient in
    case of the lawstat.dat data.
    \\

    \solution

    TODO: Derivation

<<>>=
# Bootstrap 4k times
B <- 4000

boot.fn <- function (data, indices) {
    sample <- X[indices,]
    cor(sample$LSAT, sample$GPA)
}

results <- boot(data = X, statistic = boot.fn, R = B)

results

<<p2b, fig.pos="H", fig.height=6, fig.cap="Confidence interval plots">>=

plot(results)

<<>>=

boot.ci(results, type="bca")

@

\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    From ISLR: Chapter 6, Problem 11.
    \\

    Using the Boston Housing data set (in the MASS library) complete the
    following.
    \\

    \part

    Try out some of the regression methods explore in Chapter 6 of the
    textbook. These include best subset selection, the lasso, ridge regression,
    and PCR. Present and discuss results for the approaches that you consider.
    \\

    \solution

    First let's setup a few variables that we'll use through out.

<<>>=
set.seed(1)

#
# Inital setup
#

# Grab X and Ys
x <- model.matrix(medv ~ ., Boston)[,-1]
y <- Boston$medv

# Test/Train
train <- sample(1:nrow(x), nrow(x) / 2)
test <- (-train)

x.train <- x[train,]
y.train <- y[train]

x.test <- x[test,]
y.test <- y[test]
@

\pagebreak

    \subsubsection{Best Subset Selection}

    Next let's try the best subset selection on our data:

<<eval=FALSE>>=
library('leaps')

<<>>=
set.seed(1)

p <- ncol(Boston) - 1

predict.regsubsets = function(object, newdata, id, ...) {
    form = as.formula(object$call[[2]])
    mat = model.matrix(form, newdata)
    coefi = coef(object, id = id)
    mat[, names(coefi)] %*% coefi
}

best.train <- sample(c(TRUE, FALSE),
                     nrow(Boston),
                     rep = TRUE)
best.test <- (!best.train)

fit.best <- regsubsets(medv ~ .,
                       data = Boston[best.train,],
                       nvmax = p)

cv.errors = rep(0, p)
for (i in 1:p) {
    pred = predict.regsubsets(fit.best,
                              Boston[best.test,],
                              id = i)

    cv.errors[i] = mean((Boston$medv[best.test] - pred)^2)
}

mean.best <- mean(cv.errors)
mean.best
@

<<p3b, fig.pos="H", fig.height=6, fig.cap="Best Subset Plots">>=
# Plot using the regsubsets() plots
par(mfrow = c(2, 2))

# RSS
plot(fit.best, scale = "r2")
plot(fit.best, scale = "adjr2")
plot(fit.best, scale = "Cp")
plot(fit.best, scale = "bic")
@

\pagebreak

    \subsubsection{Ridge Regression}

<<eval=FALSE>>=
library('glmnet')

<<>>=
set.seed(1)
fit.ridge.full <- glmnet(x, y,
                         alpha = 0)
fit.ridge <- cv.glmnet(x.train, y.train,
                       alpha = 0)

<<p3h, fig.pos="H", fig.height=6, fig.cap="Ridge Regression Cross Validated">>=
# Visualize how the coefficients shrink with increasing lambda
plot(fit.ridge)

<<>>=
lambda.tune <- fit.ridge$lambda.min

# Tuned lambda
lambda.tune
@

     Now let's take a look at the errors:

<<>>=
ridge.pred <- predict(fit.ridge.full,
                      s = lambda.tune,
                      newx = x.test)

mean.ridge <- mean((ridge.pred - y.test)^2)
mean.ridge

# Our final model
predict(fit.ridge.full,
        type = "coefficients",
        s = lambda.tune)
@



\pagebreak

    \subsubsection{LASSO}

<<eval=FALSE>>=
library('glmnet')

<<>>=
set.seed(1)

lambda <- seq(0, 10000)
fit.lasso.full <- glmnet(x, y,
                         alpha = 1,
                         lambda = lambda)

<<p3i, fig.pos="H", fig.height=6, fig.cap="Lasso">>=
plot(fit.lasso.full)
@

<<>>=
fit.lasso <- cv.glmnet(x.train, y.train,
                       alpha = 1)

<<p3j, fig.pos="H", fig.height=6, fig.cap="Cross Validated Lasso">>=
plot(fit.lasso)

<<>>=
lambda.tune <- fit.lasso$lambda.min

# Tuned lambda
lambda.tune
@

     Now let's take a look at the errors:

<<>>=
lasso.pred <- predict(fit.lasso.full,
                      s = lambda.tune,
                      newx = x.test)

mean.lasso <- mean((lasso.pred - y.test)^2)
mean.lasso

# Our final model
predict(fit.lasso.full,
        type = "coefficients",
        s = lambda.tune)
@

\pagebreak

    \subsubsection{PCR}

<<eval=FALSE>>=
library('pls')

<<>>=
set.seed(1)

fit.pcr.full <- pcr(medv ~ ., data = Boston,
                    scale = TRUE,
                    validation = "CV")

fit.pcr <- pcr(medv ~ ., data = Boston,
               scale = TRUE,
               subset = train,
               validation = "CV")

summary(fit.pcr)

<<p3g, fig.pos="H", fig.height=6, fig.cap="PCR Fit.">>=
# Plot validation
validationplot(fit.pcr,
               val.type = "MSEP",
               main = "Cross Validation Scores for PCR")
@

<<>>=
res <- RMSEP(fit.pcr)
pcr.best <- which.min(res$val[1,,]) - 1
pcr.best

pcr.pred <- predict(fit.pcr,
                    x.test,
                    ncomp = pcr.best)
mean.pcr <- mean((pcr.pred - y.test)^2)
mean.pcr
@

    According to the summary and the graph, the smallest RMSEP for the
    CV and adjCV, is the model with 13 components.
    \\

    This is the maximum number of components that can be in our model. Thus we
    might want to trade off for a simpler model with less components. If we
    look at the summary, it levels off around 5 components for awhile until 11
    when it drops down until it reaches 13. Picking 5 might be a good option if
    we want a simpler model.
    \\

    \part

    Propose a model (or a set of models) that seem to perform well on this data
    set, and justify your answer. Clearly explain what you will do.
    \\

    \solution

    Looking at our models, we get the following values:

    \begin{enumerate}
        \item Best Subset: \(\Sexpr{mean.best}\)
        \item Ridge: \(\Sexpr{mean.ridge}\)
        \item Lasso: \(\Sexpr{mean.lasso}\)
        \item PCR: \(\Sexpr{mean.pcr}\)
    \end{enumerate}

    As we can see, the Lasso and Ridge models are very close when you consider
    the cross validated mean squared errors. With PCR just a bit higher than
    Lasso/Ridge.  Our Best subset model is off by a large amount in comparison.
    \\

    I'd propose that we use Ridge or Lasso on this data. Given by the mean
    square errors, they perform better than the other two methods.
\end{homeworkProblem}

\pagebreak

\newcommand{\A}{{\bold A}}
\newcommand{\Left}{{\bold L}}
\newcommand{\Right}{{\bold R}}

\begin{homeworkProblem}
    Describe how you can efficiently solve the least squares linear system
    \((\X^T \X)\beta = \X^T \Y\) (i.e., by not calculating an interverse) where
    \(\X \in \R^{n \times p}\) has \(p\) linearly independent columns, \(\beta
    \in \R^p\) and \(\Y \in \R^{n \times 1}\)?
    \\

    \textit{Hint:} Think in terms of matrix decompositions (it's not SVD!). Use
    Wikipedia.
    \\

    \solution

    To improve upon the previous ways of solving the least squares linear
    system, we can use a more advanced decomposition to handle this. There are
    primarily two ways that are used. The first is called the Cholesky
    decomposition that works the fastest out of two. The second is using QR
    decomposition which is slower than Cholseky but is more numerically stable
    than Cholesky.
    \\

    We'll show the Cholesky below which goes like this:
    \begin{enumerate}
        \item Calculate and let \(A = \X^T \X\).
        \item Determine the Cholesky factorization for \(A\), let this
            equal \(\Left \Right^T\).
        \item Now on the right side, calculate \(b = \X^T \Y\).
        \item Then solve \(\Left z = b\) by forward substitution
        \item Then solve \(\Right^T \beta = z\) by back substitution.
    \end{enumerate}

    Forward/back substitution is an interative process for lower/upper
    triangular matrices.  While the link to the details can be read about
    below, I won't solve it here. Hopefully that isn't required as the question
    says to ``Describe'' and not to show/solve.

    \subsubsection{Reference}

    \begin{enumerate}
        \item Explanation of the forward/back substitution techniques for matrices:
            \url{http://en.wikipedia.org/wiki/Triangular_matrix#Forward_and_back_substitution}
    \end{enumerate}
\end{homeworkProblem}

\end{document}
