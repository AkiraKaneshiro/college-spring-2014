\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{subcaption}
\usepackage{float}

\usepackage{hyperref}
\hypersetup{colorlinks=true}

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\setlength\parindent{0pt}

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass\ (\hmwkClassInstructor\ \hmwkClassTime): \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength{\floatsep}{100pt}

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

\newenvironment{homeworkProblem}[1][]{
    \section{Problem \arabic{homeworkProblemCounter} \; \large{#1}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

\newcommand{\hmwkTitle}{Homework\ \#3}
\newcommand{\hmwkDueDate}{March 28, 2014}
\newcommand{\hmwkClass}{ComS 573}
\newcommand{\hmwkClassTime}{10am}
\newcommand{\hmwkClassInstructor}{De Brabanter}
\newcommand{\hmwkAuthorName}{Josh Davis}

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
    \vspace{0.1in}\large{\textit{Professor\ \hmwkClassInstructor\ at\ \hmwkClassTime}}
    \vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{}

\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}
\newcommand{\dx}{\mathrm{d}x}
\newcommand{\solution}{\textbf{\large Solution}}

\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\newcommand{\Std}{\mathrm{Std}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Likelihood}{\mathcal{L}}
\newcommand{\dist}[1]{\sim \mathrm{#1}}
\newcommand{\pval}{\(p\)-value}
\newcommand{\tstat}{\(t\)-statistic}

\newcommand{\X}{{\bold X}}
\newcommand{\Y}{{\bold Y}}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

\begin{document}

<<echo=FALSE, warning=FALSE, message=FALSE>>=
library('MASS')

# For Best Subset Selection
library('leaps')

# For Ridge Regression
library('glmnet')
library('genridge')

# For Bootstrapping
library('boot')

# For PCR
library('pls')
@

\maketitle

\pagebreak

\begin{homeworkProblem}
    From ISLR: Chapter 6, Problem 7.
    \\

    Suppose that \(Y_i = \beta_0 + \sum_{j = 1}^p x_{ij} \beta_j + e_i\) where
    \(e_1, \ldots, e_n\) are i.i.d. distributed from a \(\N(0, \sigma^2_e)\).
    \\

    \part

    Write out the likelihood for the data and show that it is equivalent to
    using ordinary least squares.
    \\

    \solution

    First let's remind ourselves what the ordinary least squares solution is. It
    is the set of coefficients that minimize the residual sum of squares, or:
    \[
        \text{RSS} = \sum_{i = 1}^{n} (Y_i - \hat{Y_i})^2
    \]

    where \(\hat{Y_i} = \hat{\beta_0} + \hat{\beta_1} x_{i1} + \cdots +
    \hat{\beta_p} x_{ip}\), our model.
    \\

    To show that the maximum likelihood is equal to the least squares solution
    when the error is distributed from a normal, let's look at what maximum
    likelihood means.
    \\

    Often it is useful to calculate the statistical parameters, \(\theta\), of
    a model. However this can be hard to do given data from the model, \(X\).
    Written out, the probability of getting the data we got using the
    parameters given the data or \(\Pr[\beta \mid \theta]\).
    \\

    The idea behind the likelihood is that we want to determine the probability
    in the reverse and think about given the data that we got, what is most
    likely the values of the parameters, or \(\Likelihood(\theta \mid X)\).
    Putting this together we get:
    \[
        \Likelihood(\theta \mid \beta_1, \ldots, \beta_n)
        = P(\beta_1, \ldots, \beta_n \mid \theta)
    \]

    Given that we know these i.i.d. which means we can that our joint
    probability is just equal to product of the probabilities, thus we get:
    \\
    \[
        \begin{split}
            \Likelihood(\theta \mid \beta_1, \ldots, \beta_n)
            &= P(\beta_1 \mid \theta)
            \times \cdots
            \times P(\beta_n \mid \theta)
            \\
            &= \prod_{i = 1}^{n}
            P(\beta_i \mid \theta)
        \end{split}
    \]

    Since we know that \(e_i \dist{\N}(0, \sigma^2_e)\) and that \(e_i = Y_i -
    \hat{Y_i}\), we can use the Normal distribution probability density
    function which gives us:
    \[
        \begin{split}
            \Likelihood(\theta \mid \beta_1, \ldots, \beta_n )
            &= \prod_{i = 1}^{n}
            \frac{
                1
            }{
                \sigma \sqrt{2\pi}
            }
            \exp
            \left(-
                \frac{
                    (Y_i - \hat{Y_i})^2
                }{
                    2\sigma^2
                }
            \right)
            \\
            &= \left(
                    \frac{
                        1
                    }{
                        \sigma \sqrt{2\pi}
                    }
                \right)^n
                \prod_{i = 1}^{n}
                \exp
                \left(
                    - \frac{
                        (Y_i - \hat{Y_i})^2
                    }{
                        2\sigma^2
                    }
                \right)
        \end{split}
    \]

    To simplify the expression, we can take the log of each side. This gives us
    an expression that is easier to work with and we get:
    \[
        \begin{split}
            \log \Likelihood(\theta \mid \beta_1, \ldots, \beta_n)
            &= \log
            \left[
                \left(
                    \frac{
                        1
                    }{
                        \sigma \sqrt{2\pi}
                    }
                \right)^n
                \prod_{i = 1}^{n}
                \exp
                \left(-
                    \frac{
                        (Y_i - \hat{Y_i})^2
                    }{
                        2\sigma^2
                    }
                \right)
            \right]
            \\
            &= n \log
            \left(
                \frac{
                    1
                }{
                    \sigma \sqrt{2\pi}
                }
            \right)
            + \sum_{i = 1}^{n}
            \log
            \left(
                \exp
                \left(
                    - \frac{
                        (Y_i - \hat{Y_i})^2
                    }{
                        2\sigma^2
                    }
                \right)
            \right)
            \\
            &= n \log
            \left(
                \frac{
                    1
                }{
                    \sigma \sqrt{2\pi}
                }
            \right)
            + \sum_{i = 1}^{n}
            \left(
                - \frac{
                     (Y_i - \hat{Y_i})^2
                }{
                    2\sigma^2
                }
            \right)
            \\
            &= n \log
                \left(
                    \frac{
                        1
                    }{
                        \sigma \sqrt{2\pi}
                    }
                \right)
            - \frac{1}{2\sigma^2}
            \sum_{i = 1}^{n} (Y_i - \hat{Y_i})^2
        \end{split}
    \]

    Now since we are looking to maximize our Likelihood, or \(\Likelihood\), we
    get the following:
    \[
        \begin{split}
            \log
            \Likelihood(\theta \mid \beta_1, \ldots, \beta_n)
            &= n \log
            \left(
                \frac{
                    1
                }{
                    \sigma \sqrt{2\pi}
                }
            \right)
            - \frac{
                1
            }{
                2\sigma^2
            }
            \sum_{i = 1}^{n} (Y_i - \hat{Y_i})^2
        \end{split}
    \]

    We can see that the second term is negative, thus in order to maximize \(\Likelihood\),
    we want to minimize the second term. If we remove the constant, we then want to minimize:
    \[
        \sum_{i = 1}^n (Y_i - \hat{Y_i})^2
    \]

    Well, this is nice because this is our old friend, RSS, from the beginning
    of this problem. The RSS is what the ordinary least squares aims to
    minimize. Thus we can see that when our errors, \(e_i\), are from a Normal
    distribution, the maximum likelihood is the same as ordinary least squares.
    \\

    \part

    Assume the following prior for \(\beta\): \(\beta_1, \ldots, \beta_p\), are
    i.i.d. according to a Laplace distribution with mean zero and common scale
    parameter \(c\), i.e., \(h(\beta) = \frac{1}{2c} \exp \left(- \left\| \beta
    \right\| / c \right)\).
    \\

    Write out the posterior for \(\beta\) in this setting. Argue that the LASSO
    estimate is the mode for \(\beta\) i.e., the most likely value for
    \(\beta\), under this posterior distribution. Determine the value for the
    parameter \(\lambda\) in the LASSO cost function.
    \\

    \solution

    pg. 226

\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    Suppose we estimate some statistics (e.g. median) based on a sample \(X\).
    \\

    \part

    Carefully describe how you might estimate the standard deviation of the
    median of the statistic.
    \\

    \solution

    This is the perfect situation in which we could leverage the bootstrap.
    The way we'd do this, is that we know we have a sample, \(X\). If we
    randomly sample some \(B\) number of elements \textit{with replacement}
    from \(X\) again, then we can determine a good estimate for a statistic of
    the population if we iterate this a few thousand times. An illustration of
    this is below where the median is calculated on each sample and where there
    are \(B\) samples.
    \\

    \begin{figure}[H]
        \includegraphics[width=\maxwidth]{bootstrap.pdf}
        \caption[Illustration of the Bootstrap]{Illustration of the Bootstrap.\label{fig:p2a}}
    \end{figure}


    Since we want to calculate the median of \(X\), we know that the median is
    given as \(\Pr[X < M] < 0.5\) and \(\Pr[X > M] > 0.5\). This is the value
    in which there is equal probability of being above and below it.
    \\

    \part

    Write R code that calculates the standard deviation of the median given a
    sample \(X\).
    \\

    \solution

<<>>=
# Load data & sample
lawstat <- read.table("lawstat.dat")

# Sample obvseration indices
s <- c(4,6,13,15,31,35,36,45,47,50,52,53,70,79,82)

# Given data of 15 observations
X <- lawstat[s,]
L <- length(X$GPA)


#
# Manual Bootstrap
#

# Bootstrap 4k times
B <- 4000

boot.custom <- function() {
    results <- rep(0, B)

    # I choose you, Bootstrap!
    for (i in 1:B) {
        obs <- round(L * runif(L, 0, 1))
        sample <- X[obs,]
        results[i] <- median(sample$GPA)
    }

    sd(results)
}

boot.actual <- function () {
    # Calculate the actual value
    actual <- rep(0, B)
    L <- length(lawstat$GPA)

    for (i in 1:B) {
        obs <- round(L * runif(L, 0, 1))
        sample <- lawstat[obs,]
        actual[i] <- median(sample$GPA)
    }

    # Calculated vs actual
    sd(actual)
}

#
# Using Boot Library
#

boot.median <- function () {
    boot.fn <- function (data, indices) {
        sample <- X[indices,]
        median(sample$GPA)
    }

    results <- boot(data = X, statistic = boot.fn, R = B)

    # Calculate the median from the results
    sd(results$t[,1])
}

#
# Results of using the above functions
#

# Show calculated
boot.custom()

# Show actual
boot.actual()

# Show calcualted vs actual using boot library
boot.median()
@
    \part

    Suppose you were interested in a \(100(1 - \alpha)\) (pointwise) confidence
    interval for the correlation coefficient of a sample of \(X\) and \(Y\)
    (the joint distribution of \(X\) and \(Y\) is not bivariate normal).
    Clearly explain and derive how you would do this? Write R code that
    calculates the 95\% confidence interval for the correlation coefficient in
    case of the lawstat.dat data.
    \\

    \solution

    Solution
\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    From ISLR: Chapter 6, Problem 11.
    \\

    Using the Boston Housing data set (in the MASS library) complete the
    following.
    \\

    \part

    Try out some of the regression methods explore in Chapter 6 of the
    textbook. These include best subset selection, the lasso, ridge regression,
    and PCR. Present and discuss results for the approaches that you consider.
    \\

    \solution

<<eval=FALSE>>=
library('leaps')
library('glmnet')
@

<<>>=
#
# Inital setup
#

# Ensure consistent values
set.seed(1)

# Grab X and Ys
x <- model.matrix(medv ~ ., Boston)[,-1]
y <- Boston$medv

# Test/Train
train <- sample(1:nrow(x), nrow(x) / 2)
test <- (-train)

x.train <- x[train,]
y.train <- y[train]

x.test <- x[test,]
y.test <- y[test]

#
# First, some helper functions.
#

# Show the subset summary plots
subset.basic.summary <- function (data) {
    data.summary <- summary(data)

    #
    # Basic Line Plots
    #

    par(mfrow = c(2, 2))

    # RSS
    plot(data.summary$rss,
         type = "l",
         xlab = "# of Variables",
         ylab = "Residual Sum of Squares")

    # Adj R2
    plot(data.summary$adjr2,
         type = "l",
         xlab = "# of Variables",
         ylab = "Adjusted RSquared")

    # Show the model with the largest AdjR2
    p <- which.max(data.summary$adjr2)
    points(p, data.summary$adjr2[p],
           col = "red",
           cex = 2,
           pch = 20)

    # C_p
    plot(data.summary$cp,
         type = "l",
         xlab = "# of Variables",
         ylab = "Cp")

    # Show the model with the smallest C_p
    p <- which.min(data.summary$cp)
    points(p, data.summary$cp[p],
           col = "red",
           cex = 2,
           pch = 20)

    # BIC
    plot(data.summary$bic,
         type = "l",
         xlab = "# of Variables",
         ylab = "BIC")

    # Show the model with the smallest C_p
    p <- which.min(data.summary$bic)
    points(p, data.summary$bic[p],
           col = "red",
           cex = 2,
           pch = 20)
}

# Different Kind of Plot
subset.summary <- function (data) {
    data.summary <- summary(data)

    # Plot using the regsubsets() plots
    par(mfrow = c(2, 2))

    # RSS
    plot(data, scale = "r2")
    plot(data, scale = "adjr2")
    plot(data, scale = "Cp")
    plot(data, scale = "bic")
}
@

\pagebreak

    \subsubsection{Best Subset Selection}

<<>>=
# Calculate the best subset
fit.full <- regsubsets(medv ~ ., Boston)
@

<<p3a, fig.pos="H", fig.height=6, fig.cap="Basic Line Plots.">>=
# Show the summary information/plots for it
subset.basic.summary(fit.full)
@

<<p3b, fig.pos="H", fig.height=6, fig.cap="Built-in Subset Plots.">>=
subset.summary(fit.full)
@

\pagebreak

    \subsubsection{Forward Stepwise Selection}

<<>>=
fit.forward <- regsubsets(medv ~ ., Boston, method="forward")
@

<<p3c, fig.pos="H", fig.height=6, fig.cap="Basic Line Plots.">>=
# Show the summary information/plots for it
subset.basic.summary(fit.forward)
@

<<p3d, fig.pos="H", fig.height=6, fig.cap="Built-in Subset Plots.">>=
subset.summary(fit.forward)
@

\pagebreak

    \subsubsection{Backward Stepwise Selection}

<<>>=
fit.back <- regsubsets(medv ~ ., Boston, method="backward")
@

<<p3e, fig.pos="H", fig.height=6, fig.cap="Basic Line Plots.">>=
# Show the summary information/plots for it
subset.basic.summary(fit.back)
@

<<p3f, fig.pos="H", fig.height=6, fig.cap="Built-in Subset Plots.">>=
subset.summary(fit.back)
@

\pagebreak

    \subsubsection{Ridge Regression}

<<eval=FALSE>>=
library('glmnet')

<<>>=
set.seed(1)
fit.ridge.full <- glmnet(x, y,
                         alpha = 0)
lambda <- seq(0, 10000)
fit.ridge <- cv.glmnet(x.train, y.train,
                    alpha = 0)

<<p3h, fig.pos="H", fig.height=6, fig.cap="Ridge Regression Cross Validated">>=
# Visualize how the coefficients shrink with increasing lambda
plot(fit.ridge)
<<>>=
lambda.tune <- fit.ridge$lambda.min

# Tuned lambda
lambda.tune
@

     Now let's take a look at the errors.

<<>>=
ridge.pred <- predict(fit.ridge.full,
                      s = lambda.tune,
                      newx = x.test)

mean((ridge.pred - y.test)^2)

# Our final model
predict(fit.ridge.full,
        type = "coefficients",
        s = lambda.tune)
@



\pagebreak

    \subsubsection{LASSO}

<<eval=FALSE>>=
library('glmnet')

<<>>=
set.seed(1)

fit.lasso <- glmnet(x.train, y.train,
                    alpha = 1,
                    lambda = lambda)
@

\pagebreak

    \subsubsection{PCR}

<<eval=FALSE>>=
library('pls')

<<>>=
fit.pcr <- pcr(medv ~ ., data = Boston,
               scale = TRUE,
               validation = "CV")

summary(fit.pcr)

<<p3g, fig.pos="H", fig.height=6, fig.cap="PCR Fit.">>=
# Plot validation
validationplot(fit.pcr,
               val.type = "MSEP",
               main = "Cross Validation Scores for PCR")
@

    According to the summary and the graph, the smallest RMSEP for the
    CV and adjCV, is the model with 13 components.
    \\

    This is the maximum number of components that can be in our model. Thus we
    might want to trade off for a simpler model with less components. If we
    look at the summary, it levels off around 5 components for awhile until 11
    when it drops down until it reaches 13. Picking 5 might be a good option if
    we want a simpler model.

    \part

    Propose a model (or a set of models) that seem to perform well on this data
    set, and justify your answer. Clearly explain what you will do.
    \\

    \solution

    Solution.
\end{homeworkProblem}

\pagebreak

\newcommand{\A}{{\bold A}}
\newcommand{\Left}{{\bold L}}
\newcommand{\Right}{{\bold R}}

\begin{homeworkProblem}
    Describe how you can efficiently solve the least squares linear system
    \((\X^T \X)\beta = \X^T \Y\) (i.e., by not calculating an interverse) where
    \(\X \in \R^{n \times p}\) has \(p\) linearly independent columns, \(\beta
    \in \R^p\) and \(\Y \in \R^{n \times 1}\)?
    \\

    \textit{Hint:} Think in terms of matrix decompositions (it's not SVD!). Use
    Wikipedia.
    \\

    \solution

    To improve upon the previous ways of solving the least squares linear
    system, we can use a more advanced decomposition to handle this. There are
    primarily two ways that are used. The first is called the Cholesky
    decomposition that works the fastest out of two. The second is using QR
    decomposition which is slower than Cholseky but is more numerically stable
    than Cholesky.
    \\

    We'll show the Cholesky below which goes like this:
    \begin{enumerate}
        \item Calculate and let \(A = \X^T \X\).
        \item Determine the Cholesky factorization for \(A\), let this
            equal \(\Left \Right^T\).
        \item Now on the right side, calculate \(b = \X^T \Y\).
        \item Then solve \(\Left z = b\) by forward substitution
        \item Then solve \(\Right^T \beta = z\) by back substitution.
    \end{enumerate}

    Forward/back substitution is an interative process for lower/upper triangular matrices.
    While the link to the details can be read about below, I won't solve it here. Hopefully
    that isn't required as the question says to ``Describe'' and not to show/solve.

    \subsubsection{Reference}

    \begin{enumerate}
        \item Explanation of the forward/back substitution techniques for matrices:
            \url{http://en.wikipedia.org/wiki/Triangular_matrix#Forward_and_back_substitution}
    \end{enumerate}
\end{homeworkProblem}

\end{document}
